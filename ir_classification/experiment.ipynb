{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #4\n",
    "\n",
    "**Authors:** *Ryan Ceresani*\n",
    "\n",
    "**Class:** *605.744 Information Retrieval*\n",
    "\n",
    "**Term:** *Fall 2021*\n",
    "\n",
    "This assignment submission is structured slightly different than my previous - as we are using a Notebook in place of a \"main\" application. \n",
    "The notebook provides a nice integration for intermediate code output, plotting, results and experimentation.\n",
    "\n",
    "## Expected Deliverables\n",
    "- Report outlining methodologies, tools used, parameter decisions, etc.\n",
    "- Precision, Recall, F1 score for the `\"dev\"` dataset using \"Title\" column as features.\n",
    "- Metrics for `\"dev\"` dataset using \"Title\", \"Abstract\", and \"Keywords\" as features.\n",
    "- Perform an additional non-trival experimentation.\n",
    "- Predictions for the `\"test\"` dataset.\n",
    "\n",
    "## Results First Reporting\n",
    "We will dive into all of the details later but will jump straight into the results.\n",
    "Specific details for each run will be visible in the Notebook cells below.\n",
    "\n",
    "### **Initial Attempt: SingleLinearLayerModel**\n",
    "- **Data Set Details**\n",
    "  - Initialized Embedding that was Trained only on the Training Dataset\n",
    "  - Used only \"Title\" as feature.\n",
    "- **Training Details**\n",
    "  - Epochs = 50\n",
    "  - Learning Rate = 0.001\n",
    "- **Results**\n",
    "  - Precision: 56 / (56 + 348) = **0.1386**\n",
    "  - Recall: 56 / (56 + 94) = **0.3733**\n",
    "  - F1: 2 * (0.1386 * 0.3733) / (0.1386 + 0.3733) = **0.2021**\n",
    "  - Avg Precision: 0.07113\n",
    "\n",
    "### Experimentation: GloVeMLP\n",
    "Because of bad results - did experimentation before moving on to using \"Title\" \"Abstract\" \"Keywords\"\n",
    " - **Data Set Details**\n",
    "  - Used only \"Title\", \"Abstract\", \"Keywords\" as features.\n",
    "  - GloVe 840B.300d used as vocab + initalized embeddings for model.\n",
    "- **Training Details**\n",
    "  - Same as above\n",
    "- **Results**\n",
    "  - Precision: 126 / (126 + 580) = **0.1785**\n",
    "  - Recall: 126 / (126 + 24) = **0.84**\n",
    "  - F1: 2 * (0.1785 * 0.84) / (0.1785 + 0.84) = **0.2944**\n",
    "  - AP: **0.15486346777255336**\n",
    "\n",
    "### Tensorboard Outputs\n",
    "\n",
    "![Training](images/training_tboard.png)\n",
    "![Dev](images/val_tboard.png)\n",
    "\n",
    "## **Overall Methodology**\n",
    "Before getting into code specifics, we will address some overall elements used during this assignment. The main approach I am choosing to perform **Text Classification** is through deep learning.\n",
    "\n",
    "### Open Source Libraries\n",
    "To facility deep learning, this experiment makes heavy use of `PyTorch` and `torchtext` for doing the underlying operations.  Additionally, `scikit-learn` is useful for their `metrics` library which offers useful calculation ability for a variety of classifation metrics (whether you use one of their estimators or not.)  \n",
    "- **PyTorch**: This is the heavy-lifter for a number of backing abstract classes performing a variety of functions.\n",
    "- **Torchtext**: Extension off the official PyTorch to provide text based utilities.\n",
    "   -  `vocab`: This generates a `Vocabulary` object from an iterator which can be used to map words to indices, converting tokens into values.\n",
    "   -  `utils`: It also has some built in utilities for tokenizing and creating ngrams.\n",
    "- **scikit-learn**: Used for the `metrics` library to generate scores and repots.\n",
    "\n",
    "### **Dataset**\n",
    "Systematic Review\n",
    "\n",
    "- NOTE: The Document **hash:a8113f0b-6561-3178-8c2d-7b4ebac229ff** contained an odd sequence of characters in UTF8 at the beginning of the \"Article\" section which caused problems for the python stdlib `csv_reader`. The value was sanitized to remove the characters (`\",`) - which would be removed in tokenization anyway. \n",
    "\n",
    "#### Dataset Challenges\n",
    "A primary challenge imposed by this dataset is the vast class imbalance within training. (30:1 negative skew)\n",
    "\n",
    "To counteract this, two approaches considered were:\n",
    "1. The `DataLoader` used a `WeightedRandomSampler` which was weighted to allow the random sampler topopulate batches with a statistically equal chance for both classes.\n",
    "2. The chosen loss function (`CrossEntropyLoss`) used inverse class frequency weighting to encourage learning on the minority class.\n",
    "   - This ended up providing less benefit and the combination of both was worse.\n",
    "\n",
    "\n",
    "### Models\n",
    "After numerous experimentation, two models were used during the course of this. Both used an `EmbeddingBag` instead of a typical embedding. It provides a lot of functional benefits but does remove the ability to consider sequences of words. So the model loses all context of the sentences themselves.\n",
    "\n",
    "The actual code for the models will be appended to the end of the notebook.\n",
    "\n",
    "#### Simple Linear Model\n",
    "The first draft model used the embedding bag and a single Linear Layer. The results were not very promising and it was abandoned. (It was tested on the TAK features as well with limited change.)\n",
    "\n",
    "#### MLP Model\n",
    "In an effort to increase the generalization, which was a problem with the first model, I created a straight-forward MLP. The inclusion of activations, in this case, `GELU`, provided for non-linearity and seemed to increase performance. \n",
    "- 3 fully-connected layers\n",
    "- GELU Activation\n",
    "- Dropout during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Walkthrough\n",
    "\n",
    "We will now walk through the experiment notebook to see results in action.\n",
    "\n",
    "### Imports\n",
    "\n",
    "The open source and custom modules used are imported first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ir_classification import datasets, models\n",
    "from ir_classification import vocab as ir_vocab\n",
    "from ir_classification import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Training and Validation Datasets\n",
    "\n",
    "This first time through we do not use GloVe, just the original vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ir-classification-_Pgcz6ju-py3.9\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "datafield_map = {\"assessment\": 0, \"doc_id\": 1, \"title\": 2, \"authors\": 3, \"journal\": 4, \"issn\": 5, \"year\": 6, \"language\": 7, \"abstract\": 8, \"keywords\": 9}\n",
    "data_columns = [datafield_map[\"title\"]]\n",
    "ngrams = 1\n",
    "batch_size = 64\n",
    "\n",
    "# Create vocab from the training data.\n",
    "vocab = ir_vocab.create_vocab_from_tsv(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns, ngrams=ngrams)\n",
    "\n",
    "# Load the TSV into datasets with the appropriate feature columns.\n",
    "train_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns)\n",
    "val_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.dev.shuf.tsv\", data_columns)\n",
    "\n",
    "# Create the transforms for the dataloader to appropriately format the contents of the files.\n",
    "label_transform = lambda x: x if x > 0 else 0\n",
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "text_transform = lambda x: list(ngrams_iterator(tokenizer(x), ngrams))\n",
    "\n",
    "# Instantiate the dataloaders.\n",
    "train_dataloader = datasets.create_torch_dataloader(train_dataset, vocab,  label_transform, text_transform, weighted=True, batch_size=batch_size)\n",
    "val_dataloader = datasets.create_torch_dataloader(val_dataset, vocab, label_transform, text_transform, weighted=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "vocab_size = len(vocab) # from vocab created earlier.\n",
    "embedding_size = 64\n",
    "hidden_layer_size = 100\n",
    "\n",
    "# Enable compatability when training with GPU enabled devices.  \n",
    "# (Some development work was done in Google Colab with GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.EmbeddingBagLinearModel(vocab_size, embedding_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the top-level training loop\n",
    "\n",
    "The custom code was meant to handle the individual `step` and `epoch` levels generically.\n",
    "This setup should let us change the components experimentally in cells like below without much other hassle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "           Epoch 0: 100%|██████████| 340/340 [00:08<00:00, 40.23 batch/s, accurracy=1, loss=0.459]\n",
      "     Validation: 0: 100%|██████████| 76/76 [00:00<00:00, 89.42 batch/s, accurracy=0.82, loss=0.413]\n",
      "           Epoch 1: 100%|██████████| 340/340 [00:07<00:00, 48.20 batch/s, accurracy=1, loss=0.268]\n",
      "     Validation: 1: 100%|██████████| 76/76 [00:00<00:00, 140.39 batch/s, accurracy=0.92, loss=0.266]\n",
      "           Epoch 2: 100%|██████████| 340/340 [00:06<00:00, 49.98 batch/s, accurracy=1, loss=0.126]\n",
      "     Validation: 2: 100%|██████████| 76/76 [00:00<00:00, 127.11 batch/s, accurracy=0.74, loss=0.514]\n",
      "           Epoch 3: 100%|██████████| 340/340 [00:07<00:00, 47.85 batch/s, accurracy=1, loss=0.0499]\n",
      "     Validation: 3: 100%|██████████| 76/76 [00:00<00:00, 140.08 batch/s, accurracy=0.86, loss=0.308]\n",
      "           Epoch 4: 100%|██████████| 340/340 [00:06<00:00, 50.22 batch/s, accurracy=1, loss=0.0969]\n",
      "     Validation: 4: 100%|██████████| 76/76 [00:00<00:00, 132.20 batch/s, accurracy=0.92, loss=0.355]\n",
      "           Epoch 5: 100%|██████████| 340/340 [00:06<00:00, 50.21 batch/s, accurracy=1, loss=0.0385]\n",
      "     Validation: 5: 100%|██████████| 76/76 [00:00<00:00, 124.52 batch/s, accurracy=0.84, loss=0.412]\n",
      "           Epoch 6: 100%|██████████| 340/340 [00:06<00:00, 50.09 batch/s, accurracy=1, loss=0.021]\n",
      "     Validation: 6: 100%|██████████| 76/76 [00:00<00:00, 138.31 batch/s, accurracy=0.9, loss=0.227]\n",
      "           Epoch 7: 100%|██████████| 340/340 [00:06<00:00, 49.07 batch/s, accurracy=1, loss=0.225]\n",
      "     Validation: 7: 100%|██████████| 76/76 [00:00<00:00, 130.48 batch/s, accurracy=0.88, loss=0.282]\n",
      "           Epoch 8: 100%|██████████| 340/340 [00:06<00:00, 49.27 batch/s, accurracy=1, loss=0.119]\n",
      "     Validation: 8: 100%|██████████| 76/76 [00:00<00:00, 152.10 batch/s, accurracy=0.86, loss=0.351]\n",
      "           Epoch 9: 100%|██████████| 340/340 [00:07<00:00, 48.11 batch/s, accurracy=1, loss=0.0589]\n",
      "     Validation: 9: 100%|██████████| 76/76 [00:00<00:00, 127.25 batch/s, accurracy=0.88, loss=0.314]\n",
      "          Epoch 10: 100%|██████████| 340/340 [00:06<00:00, 54.11 batch/s, accurracy=1, loss=0.0574]\n",
      "    Validation: 10: 100%|██████████| 76/76 [00:00<00:00, 154.88 batch/s, accurracy=0.8, loss=0.403]\n",
      "          Epoch 11: 100%|██████████| 340/340 [00:05<00:00, 56.80 batch/s, accurracy=1, loss=0.254]\n",
      "    Validation: 11: 100%|██████████| 76/76 [00:00<00:00, 163.88 batch/s, accurracy=0.96, loss=0.216]\n",
      "          Epoch 12: 100%|██████████| 340/340 [00:05<00:00, 57.47 batch/s, accurracy=1, loss=0.156]\n",
      "    Validation: 12: 100%|██████████| 76/76 [00:00<00:00, 153.47 batch/s, accurracy=0.82, loss=0.487]\n",
      "          Epoch 13: 100%|██████████| 340/340 [00:05<00:00, 57.83 batch/s, accurracy=1, loss=0.0658]\n",
      "    Validation: 13: 100%|██████████| 76/76 [00:00<00:00, 154.07 batch/s, accurracy=0.92, loss=0.2]\n",
      "          Epoch 14: 100%|██████████| 340/340 [00:05<00:00, 56.98 batch/s, accurracy=1, loss=0.0601]\n",
      "    Validation: 14: 100%|██████████| 76/76 [00:00<00:00, 147.39 batch/s, accurracy=0.94, loss=0.237]\n",
      "          Epoch 15: 100%|██████████| 340/340 [00:06<00:00, 53.63 batch/s, accurracy=1, loss=0.033]\n",
      "    Validation: 15: 100%|██████████| 76/76 [00:00<00:00, 120.75 batch/s, accurracy=0.92, loss=0.215]\n",
      "          Epoch 16: 100%|██████████| 340/340 [00:07<00:00, 47.01 batch/s, accurracy=1, loss=0.00694]\n",
      "    Validation: 16: 100%|██████████| 76/76 [00:00<00:00, 138.89 batch/s, accurracy=0.86, loss=0.561]\n",
      "          Epoch 17: 100%|██████████| 340/340 [00:06<00:00, 50.09 batch/s, accurracy=1, loss=0.0298]\n",
      "    Validation: 17: 100%|██████████| 76/76 [00:00<00:00, 124.71 batch/s, accurracy=0.94, loss=0.0985]\n",
      "          Epoch 18: 100%|██████████| 340/340 [00:06<00:00, 49.90 batch/s, accurracy=0.75, loss=0.295]\n",
      "    Validation: 18: 100%|██████████| 76/76 [00:00<00:00, 139.97 batch/s, accurracy=0.86, loss=0.52]\n",
      "          Epoch 19: 100%|██████████| 340/340 [00:06<00:00, 50.88 batch/s, accurracy=1, loss=0.033]\n",
      "    Validation: 19: 100%|██████████| 76/76 [00:00<00:00, 116.70 batch/s, accurracy=0.88, loss=0.376]\n",
      "          Epoch 20: 100%|██████████| 340/340 [00:06<00:00, 49.02 batch/s, accurracy=1, loss=0.0197]\n",
      "    Validation: 20: 100%|██████████| 76/76 [00:00<00:00, 116.72 batch/s, accurracy=0.98, loss=0.0836]\n",
      "          Epoch 21: 100%|██████████| 340/340 [00:06<00:00, 50.68 batch/s, accurracy=1, loss=0.075]\n",
      "    Validation: 21: 100%|██████████| 76/76 [00:00<00:00, 131.27 batch/s, accurracy=0.9, loss=0.379]\n",
      "          Epoch 22: 100%|██████████| 340/340 [00:06<00:00, 50.23 batch/s, accurracy=1, loss=0.0487]\n",
      "    Validation: 22: 100%|██████████| 76/76 [00:00<00:00, 125.02 batch/s, accurracy=0.84, loss=0.369]\n",
      "          Epoch 23: 100%|██████████| 340/340 [00:06<00:00, 51.22 batch/s, accurracy=1, loss=0.0527]\n",
      "    Validation: 23: 100%|██████████| 76/76 [00:00<00:00, 136.93 batch/s, accurracy=0.94, loss=0.227]\n",
      "          Epoch 24: 100%|██████████| 340/340 [00:06<00:00, 51.04 batch/s, accurracy=1, loss=0.0666]\n",
      "    Validation: 24: 100%|██████████| 76/76 [00:00<00:00, 132.07 batch/s, accurracy=0.96, loss=0.583]\n",
      "          Epoch 25: 100%|██████████| 340/340 [00:06<00:00, 51.19 batch/s, accurracy=1, loss=0.0122]\n",
      "    Validation: 25: 100%|██████████| 76/76 [00:00<00:00, 138.55 batch/s, accurracy=0.94, loss=0.159]\n",
      "          Epoch 26: 100%|██████████| 340/340 [00:06<00:00, 48.99 batch/s, accurracy=1, loss=0.0256]\n",
      "    Validation: 26: 100%|██████████| 76/76 [00:00<00:00, 146.68 batch/s, accurracy=0.86, loss=0.325]\n",
      "          Epoch 27: 100%|██████████| 340/340 [00:06<00:00, 51.50 batch/s, accurracy=1, loss=0.0169]\n",
      "    Validation: 27: 100%|██████████| 76/76 [00:00<00:00, 137.55 batch/s, accurracy=0.88, loss=0.251]\n",
      "          Epoch 28: 100%|██████████| 340/340 [00:06<00:00, 50.05 batch/s, accurracy=0.75, loss=1.6]\n",
      "    Validation: 28: 100%|██████████| 76/76 [00:00<00:00, 136.20 batch/s, accurracy=0.82, loss=0.464]\n",
      "          Epoch 29: 100%|██████████| 340/340 [00:06<00:00, 49.98 batch/s, accurracy=1, loss=0.0767]\n",
      "    Validation: 29: 100%|██████████| 76/76 [00:00<00:00, 123.22 batch/s, accurracy=0.84, loss=0.693]\n",
      "          Epoch 30: 100%|██████████| 340/340 [00:07<00:00, 48.35 batch/s, accurracy=1, loss=0.0422]\n",
      "    Validation: 30: 100%|██████████| 76/76 [00:00<00:00, 132.65 batch/s, accurracy=0.96, loss=0.215]\n",
      "          Epoch 31: 100%|██████████| 340/340 [00:06<00:00, 50.11 batch/s, accurracy=1, loss=0.0102]\n",
      "    Validation: 31: 100%|██████████| 76/76 [00:00<00:00, 134.87 batch/s, accurracy=0.9, loss=0.206]\n",
      "          Epoch 32: 100%|██████████| 340/340 [00:06<00:00, 50.93 batch/s, accurracy=1, loss=0.154]\n",
      "    Validation: 32: 100%|██████████| 76/76 [00:00<00:00, 142.81 batch/s, accurracy=0.94, loss=0.318]\n",
      "          Epoch 33: 100%|██████████| 340/340 [00:06<00:00, 50.37 batch/s, accurracy=1, loss=0.0167]\n",
      "    Validation: 33: 100%|██████████| 76/76 [00:00<00:00, 133.03 batch/s, accurracy=0.9, loss=0.421]\n",
      "          Epoch 34: 100%|██████████| 340/340 [00:06<00:00, 50.19 batch/s, accurracy=1, loss=0.0273]\n",
      "    Validation: 34: 100%|██████████| 76/76 [00:00<00:00, 139.28 batch/s, accurracy=0.92, loss=0.194]\n",
      "          Epoch 35: 100%|██████████| 340/340 [00:06<00:00, 50.29 batch/s, accurracy=1, loss=0.00164]\n",
      "    Validation: 35: 100%|██████████| 76/76 [00:00<00:00, 128.29 batch/s, accurracy=0.9, loss=0.422]\n",
      "          Epoch 36: 100%|██████████| 340/340 [00:06<00:00, 49.49 batch/s, accurracy=0.75, loss=0.474]\n",
      "    Validation: 36: 100%|██████████| 76/76 [00:00<00:00, 140.86 batch/s, accurracy=0.9, loss=0.539]\n",
      "          Epoch 37: 100%|██████████| 340/340 [00:06<00:00, 50.29 batch/s, accurracy=1, loss=0.125]\n",
      "    Validation: 37: 100%|██████████| 76/76 [00:00<00:00, 142.97 batch/s, accurracy=0.9, loss=0.23]\n",
      "          Epoch 38: 100%|██████████| 340/340 [00:06<00:00, 51.02 batch/s, accurracy=1, loss=0.0303]\n",
      "    Validation: 38: 100%|██████████| 76/76 [00:00<00:00, 125.85 batch/s, accurracy=0.96, loss=0.105]\n",
      "          Epoch 39: 100%|██████████| 340/340 [00:06<00:00, 49.23 batch/s, accurracy=1, loss=0.0738]\n",
      "    Validation: 39: 100%|██████████| 76/76 [00:00<00:00, 130.64 batch/s, accurracy=0.92, loss=0.452]\n",
      "          Epoch 40: 100%|██████████| 340/340 [00:06<00:00, 50.23 batch/s, accurracy=1, loss=0.0343]\n",
      "    Validation: 40: 100%|██████████| 76/76 [00:00<00:00, 126.79 batch/s, accurracy=0.9, loss=0.202]\n",
      "          Epoch 41: 100%|██████████| 340/340 [00:06<00:00, 49.61 batch/s, accurracy=1, loss=0.0327]\n",
      "    Validation: 41: 100%|██████████| 76/76 [00:00<00:00, 124.90 batch/s, accurracy=0.96, loss=0.18]\n",
      "          Epoch 42: 100%|██████████| 340/340 [00:06<00:00, 49.66 batch/s, accurracy=1, loss=0.0553]\n",
      "    Validation: 42: 100%|██████████| 76/76 [00:00<00:00, 138.55 batch/s, accurracy=0.94, loss=0.202]\n",
      "          Epoch 43: 100%|██████████| 340/340 [00:06<00:00, 49.40 batch/s, accurracy=1, loss=0.022]\n",
      "    Validation: 43: 100%|██████████| 76/76 [00:00<00:00, 144.19 batch/s, accurracy=0.88, loss=0.432]\n",
      "          Epoch 44: 100%|██████████| 340/340 [00:06<00:00, 51.24 batch/s, accurracy=1, loss=0.0275]\n",
      "    Validation: 44: 100%|██████████| 76/76 [00:00<00:00, 118.50 batch/s, accurracy=0.96, loss=0.157]\n",
      "          Epoch 45: 100%|██████████| 340/340 [00:07<00:00, 48.55 batch/s, accurracy=1, loss=0.0225]\n",
      "    Validation: 45: 100%|██████████| 76/76 [00:00<00:00, 117.10 batch/s, accurracy=0.96, loss=0.0771]\n",
      "          Epoch 46: 100%|██████████| 340/340 [00:06<00:00, 49.09 batch/s, accurracy=1, loss=0.0735]\n",
      "    Validation: 46: 100%|██████████| 76/76 [00:00<00:00, 137.30 batch/s, accurracy=0.96, loss=0.105]\n",
      "          Epoch 47: 100%|██████████| 340/340 [00:07<00:00, 47.97 batch/s, accurracy=1, loss=0.0176]\n",
      "    Validation: 47: 100%|██████████| 76/76 [00:00<00:00, 133.75 batch/s, accurracy=0.88, loss=0.795]\n",
      "          Epoch 48: 100%|██████████| 340/340 [00:07<00:00, 48.42 batch/s, accurracy=1, loss=0.00247]\n",
      "    Validation: 48: 100%|██████████| 76/76 [00:00<00:00, 121.33 batch/s, accurracy=0.96, loss=0.261]\n",
      "          Epoch 49: 100%|██████████| 340/340 [00:07<00:00, 47.34 batch/s, accurracy=1, loss=0.00522]\n",
      "    Validation: 49: 100%|██████████| 76/76 [00:00<00:00, 127.11 batch/s, accurracy=0.92, loss=0.217]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create the loss function weighted to inverse class distribution\n",
    "# loss_function = torch.nn.CrossEntropyLoss(weight=train_dataset.class_weights)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate a Stochastic Gradient Descent optimizer and \"Auto\" Learning Rate schedule.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# Tensorboard writing utility class.\n",
    "log_dir = \"runs/SimpleLinearModel\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Perform Training\n",
    "for i in range(EPOCHS):\n",
    "    start_iter = len(train_dataloader) * i\n",
    "    train.train_epoch(i, model, optimizer, loss_function, train_dataloader, start_iter=start_iter, writer=writer)\n",
    "    validation_results = train.evaluate_epoch(i, model, loss_function, val_dataloader, writer)\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"model_weights/title_only_SLM_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Metrics on the Dev Set\n",
    "Here we recreate the `dev` dataloader to go a single document at a time.\n",
    "\n",
    "We use the less robust `predict` method so we can explicitly show the values and calculations being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataloader = datasets.create_torch_dataloader(val_dataset, vocab,  label_transform, text_transform, weighted=False, batch_size=1)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "labels = []\n",
    "for batch in dev_dataloader:\n",
    "    label, text, *_ = batch\n",
    "    pred_label = train.predict(model, text)\n",
    "    preds.append(pred_label)\n",
    "    labels.append(label.cpu().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Confusion Matrix\n",
    "\n",
    "We can use the confusion matrix to make it easy to visualize the values for the metrics.  \n",
    "       \n",
    "\n",
    "       \n",
    "|   | P0  | P1  |\n",
    "|---|----|----|\n",
    "| A0 | TN | FP |\n",
    "| A1 | FN | TP |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4352  348]\n",
      " [  94   56]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, average_precision_score\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Precision, Recall, F1-Score on Dev Set\n",
    "We use a confusion matrix to make it easy to map out the values for true positive, true negative, false positive, and false negative.\n",
    "\n",
    "- Precision = tp / (tp + fp)\n",
    "- Recall = tp / (tp + fn)\n",
    "- F1 Score =  2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 56 / (56 + 348) = 0.1386\n",
      "Recall: 56 / (56 + 94) = 0.3733\n",
      "F1: 2 * (0.1386 * 0.3733) / (0.1386 + 0.3733) = 0.2021\n",
      "AP: 0.07113061821646083\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = cm.ravel() # Extract the components\n",
    "\n",
    "# Calculate and print Precision\n",
    "precision_string = f\"{tp} / ({tp} + {fp})\"\n",
    "precision = round(eval(precision_string), 4)\n",
    "print(f\"Precision: {precision_string} = {precision}\")\n",
    "\n",
    "# Calculate and print Recall\n",
    "recall_string = f\"{tp} / ({tp} + {fn})\"\n",
    "recall = round(eval(recall_string), 4)\n",
    "print(f\"Recall: {recall_string} = {recall}\")\n",
    "\n",
    "# Calculate and print F1 Score\n",
    "f1_string = f\"2 * ({precision} * {recall}) / ({precision} + {recall})\"\n",
    "f1 = eval(f1_string)\n",
    "print(f\"F1: {f1_string} = {round(f1, 4)}\")\n",
    "\n",
    "print(f\"AP: {average_precision_score(labels, preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Methods\n",
    "\n",
    "Because of the poor performance, we are going to update the vocab and model.\n",
    "\n",
    "- Use GloVe embeddings within the model as well as the basis for the vocabulary.\n",
    "- Update the model to be a 3-layer MLP with Activation to introduce non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat Experiment with \"Title\", \"Abstract\", and \"Keyword\" Data\n",
    "\n",
    "We will keep everything exactly the same for setup, changing only the things needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_columns = [\"title\", \"abstract\", \"keywords\"]\n",
    "data_columns = [datafield_map[col] for col in use_columns]\n",
    "ngrams = 1\n",
    "\n",
    "glove = ir_vocab.create_glove_with_unk_vector()\n",
    "vocab = ir_vocab.create_vocab_from_glove(glove)\n",
    "\n",
    "# Load the TSV into datasets with the appropriate feature columns.\n",
    "train_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns)\n",
    "val_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.dev.shuf.tsv\", data_columns)\n",
    "\n",
    "# Instantiate the dataloaders.\n",
    "train_dataloader = datasets.create_torch_dataloader(train_dataset, vocab,  label_transform, text_transform, weighted=True, batch_size=batch_size)\n",
    "val_dataloader = datasets.create_torch_dataloader(val_dataset, vocab,  label_transform, text_transform, weighted=False, batch_size=batch_size)\n",
    "\n",
    "# Create Model\n",
    "model = models.EmbeddingBagMLPModel(num_class=num_classes, hidden_layer_size=100, embedding_vectors=glove.vectors, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "           Epoch 0: 100%|██████████| 340/340 [00:14<00:00, 23.62 batch/s, accurracy=0.75, loss=0.994]\n",
      "     Validation: 0: 100%|██████████| 76/76 [00:02<00:00, 29.22 batch/s, accurracy=0.78, loss=0.357]\n",
      "           Epoch 1: 100%|██████████| 340/340 [00:13<00:00, 24.55 batch/s, accurracy=0.75, loss=0.299]\n",
      "     Validation: 1: 100%|██████████| 76/76 [00:02<00:00, 29.60 batch/s, accurracy=0.9, loss=0.218]\n",
      "           Epoch 2: 100%|██████████| 340/340 [00:13<00:00, 24.98 batch/s, accurracy=0.25, loss=1.29]\n",
      "     Validation: 2: 100%|██████████| 76/76 [00:02<00:00, 28.30 batch/s, accurracy=0.76, loss=0.519]\n",
      "           Epoch 3: 100%|██████████| 340/340 [00:13<00:00, 25.43 batch/s, accurracy=1, loss=0.179]\n",
      "     Validation: 3: 100%|██████████| 76/76 [00:02<00:00, 25.76 batch/s, accurracy=0.84, loss=0.283]\n",
      "           Epoch 4: 100%|██████████| 340/340 [00:13<00:00, 26.04 batch/s, accurracy=0.75, loss=0.538]\n",
      "     Validation: 4: 100%|██████████| 76/76 [00:02<00:00, 29.29 batch/s, accurracy=0.84, loss=0.319]\n",
      "           Epoch 5: 100%|██████████| 340/340 [00:12<00:00, 26.30 batch/s, accurracy=0.75, loss=0.337]\n",
      "     Validation: 5: 100%|██████████| 76/76 [00:02<00:00, 29.53 batch/s, accurracy=0.78, loss=0.36]\n",
      "           Epoch 6: 100%|██████████| 340/340 [00:12<00:00, 26.23 batch/s, accurracy=1, loss=0.0418]\n",
      "     Validation: 6: 100%|██████████| 76/76 [00:02<00:00, 28.43 batch/s, accurracy=0.86, loss=0.319]\n",
      "           Epoch 7: 100%|██████████| 340/340 [00:13<00:00, 25.90 batch/s, accurracy=1, loss=0.137]\n",
      "     Validation: 7: 100%|██████████| 76/76 [00:02<00:00, 27.00 batch/s, accurracy=0.84, loss=0.294]\n",
      "           Epoch 8: 100%|██████████| 340/340 [00:13<00:00, 25.74 batch/s, accurracy=0.75, loss=0.296]\n",
      "     Validation: 8: 100%|██████████| 76/76 [00:02<00:00, 29.07 batch/s, accurracy=0.82, loss=0.328]\n",
      "           Epoch 9: 100%|██████████| 340/340 [00:12<00:00, 26.52 batch/s, accurracy=0.75, loss=0.298]\n",
      "     Validation: 9: 100%|██████████| 76/76 [00:02<00:00, 29.01 batch/s, accurracy=0.84, loss=0.322]\n",
      "          Epoch 10: 100%|██████████| 340/340 [00:13<00:00, 25.97 batch/s, accurracy=1, loss=0.0966]\n",
      "    Validation: 10: 100%|██████████| 76/76 [00:02<00:00, 27.86 batch/s, accurracy=0.74, loss=0.5]\n",
      "          Epoch 11: 100%|██████████| 340/340 [00:13<00:00, 25.77 batch/s, accurracy=1, loss=0.0825]\n",
      "    Validation: 11: 100%|██████████| 76/76 [00:02<00:00, 28.20 batch/s, accurracy=0.92, loss=0.196]\n",
      "          Epoch 12: 100%|██████████| 340/340 [00:13<00:00, 25.55 batch/s, accurracy=0.75, loss=0.367]\n",
      "    Validation: 12: 100%|██████████| 76/76 [00:02<00:00, 25.81 batch/s, accurracy=0.86, loss=0.205]\n",
      "          Epoch 13: 100%|██████████| 340/340 [00:13<00:00, 25.51 batch/s, accurracy=1, loss=0.129]\n",
      "    Validation: 13: 100%|██████████| 76/76 [00:02<00:00, 27.89 batch/s, accurracy=0.82, loss=0.349]\n",
      "          Epoch 14: 100%|██████████| 340/340 [00:13<00:00, 25.17 batch/s, accurracy=1, loss=0.134]\n",
      "    Validation: 14: 100%|██████████| 76/76 [00:02<00:00, 26.59 batch/s, accurracy=0.82, loss=0.279]\n",
      "          Epoch 15: 100%|██████████| 340/340 [00:13<00:00, 25.09 batch/s, accurracy=0.75, loss=0.419]\n",
      "    Validation: 15: 100%|██████████| 76/76 [00:03<00:00, 22.45 batch/s, accurracy=0.86, loss=0.304]\n",
      "          Epoch 16: 100%|██████████| 340/340 [00:15<00:00, 22.66 batch/s, accurracy=1, loss=0.00779]\n",
      "    Validation: 16: 100%|██████████| 76/76 [00:03<00:00, 22.72 batch/s, accurracy=0.82, loss=0.282]\n",
      "          Epoch 17: 100%|██████████| 340/340 [00:15<00:00, 22.38 batch/s, accurracy=1, loss=0.125]\n",
      "    Validation: 17: 100%|██████████| 76/76 [00:03<00:00, 23.23 batch/s, accurracy=0.8, loss=0.409]\n",
      "          Epoch 18: 100%|██████████| 340/340 [00:15<00:00, 21.93 batch/s, accurracy=0.75, loss=0.758]\n",
      "    Validation: 18: 100%|██████████| 76/76 [00:03<00:00, 24.47 batch/s, accurracy=0.82, loss=0.38]\n",
      "          Epoch 19: 100%|██████████| 340/340 [00:15<00:00, 22.55 batch/s, accurracy=1, loss=0.168]\n",
      "    Validation: 19: 100%|██████████| 76/76 [00:03<00:00, 24.59 batch/s, accurracy=0.88, loss=0.263]\n",
      "          Epoch 20: 100%|██████████| 340/340 [00:14<00:00, 23.27 batch/s, accurracy=0.75, loss=0.746]\n",
      "    Validation: 20: 100%|██████████| 76/76 [00:03<00:00, 24.57 batch/s, accurracy=0.82, loss=0.306]\n",
      "          Epoch 21: 100%|██████████| 340/340 [00:14<00:00, 22.98 batch/s, accurracy=0.75, loss=0.327]\n",
      "    Validation: 21: 100%|██████████| 76/76 [00:02<00:00, 26.71 batch/s, accurracy=0.94, loss=0.211]\n",
      "          Epoch 22: 100%|██████████| 340/340 [00:13<00:00, 24.95 batch/s, accurracy=1, loss=0.0574]\n",
      "    Validation: 22: 100%|██████████| 76/76 [00:02<00:00, 28.22 batch/s, accurracy=0.8, loss=0.344]\n",
      "          Epoch 23: 100%|██████████| 340/340 [00:13<00:00, 25.97 batch/s, accurracy=1, loss=0.0406]\n",
      "    Validation: 23: 100%|██████████| 76/76 [00:02<00:00, 25.36 batch/s, accurracy=0.88, loss=0.209]\n",
      "          Epoch 24: 100%|██████████| 340/340 [00:14<00:00, 23.73 batch/s, accurracy=1, loss=0.0563]\n",
      "    Validation: 24: 100%|██████████| 76/76 [00:02<00:00, 25.59 batch/s, accurracy=0.9, loss=0.143]\n",
      "          Epoch 25: 100%|██████████| 340/340 [00:13<00:00, 25.84 batch/s, accurracy=0.75, loss=0.245]\n",
      "    Validation: 25: 100%|██████████| 76/76 [00:02<00:00, 27.95 batch/s, accurracy=0.9, loss=0.157]\n",
      "          Epoch 26: 100%|██████████| 340/340 [00:13<00:00, 25.51 batch/s, accurracy=1, loss=0.153]\n",
      "    Validation: 26: 100%|██████████| 76/76 [00:02<00:00, 27.60 batch/s, accurracy=0.92, loss=0.188]\n",
      "          Epoch 27: 100%|██████████| 340/340 [00:13<00:00, 25.56 batch/s, accurracy=1, loss=0.0932]\n",
      "    Validation: 27: 100%|██████████| 76/76 [00:02<00:00, 27.51 batch/s, accurracy=0.96, loss=0.104]\n",
      "          Epoch 28: 100%|██████████| 340/340 [00:13<00:00, 25.37 batch/s, accurracy=1, loss=0.0429]\n",
      "    Validation: 28: 100%|██████████| 76/76 [00:02<00:00, 27.24 batch/s, accurracy=0.8, loss=0.439]\n",
      "          Epoch 29: 100%|██████████| 340/340 [00:13<00:00, 25.26 batch/s, accurracy=1, loss=0.0793]\n",
      "    Validation: 29: 100%|██████████| 76/76 [00:02<00:00, 28.76 batch/s, accurracy=0.88, loss=0.239]\n",
      "          Epoch 30: 100%|██████████| 340/340 [00:13<00:00, 25.40 batch/s, accurracy=1, loss=0.117]\n",
      "    Validation: 30: 100%|██████████| 76/76 [00:02<00:00, 28.14 batch/s, accurracy=0.92, loss=0.136]\n",
      "          Epoch 31: 100%|██████████| 340/340 [00:13<00:00, 25.16 batch/s, accurracy=0.75, loss=0.479]\n",
      "    Validation: 31: 100%|██████████| 76/76 [00:02<00:00, 28.87 batch/s, accurracy=0.9, loss=0.213]\n",
      "          Epoch 32: 100%|██████████| 340/340 [00:13<00:00, 25.64 batch/s, accurracy=1, loss=0.038]\n",
      "    Validation: 32: 100%|██████████| 76/76 [00:03<00:00, 22.90 batch/s, accurracy=0.88, loss=0.298]\n",
      "          Epoch 33: 100%|██████████| 340/340 [00:14<00:00, 24.07 batch/s, accurracy=0.75, loss=0.246]\n",
      "    Validation: 33: 100%|██████████| 76/76 [00:02<00:00, 27.83 batch/s, accurracy=0.82, loss=0.457]\n",
      "          Epoch 34: 100%|██████████| 340/340 [00:13<00:00, 24.65 batch/s, accurracy=1, loss=0.0358]\n",
      "    Validation: 34: 100%|██████████| 76/76 [00:03<00:00, 24.36 batch/s, accurracy=0.86, loss=0.226]\n",
      "          Epoch 35: 100%|██████████| 340/340 [00:14<00:00, 24.02 batch/s, accurracy=1, loss=0.0942]\n",
      "    Validation: 35: 100%|██████████| 76/76 [00:02<00:00, 27.15 batch/s, accurracy=0.9, loss=0.174]\n",
      "          Epoch 36: 100%|██████████| 340/340 [00:13<00:00, 25.65 batch/s, accurracy=1, loss=0.0324]\n",
      "    Validation: 36: 100%|██████████| 76/76 [00:02<00:00, 26.43 batch/s, accurracy=0.8, loss=0.427]\n",
      "          Epoch 37: 100%|██████████| 340/340 [00:13<00:00, 25.20 batch/s, accurracy=1, loss=0.27]\n",
      "    Validation: 37: 100%|██████████| 76/76 [00:02<00:00, 26.31 batch/s, accurracy=0.9, loss=0.165]\n",
      "          Epoch 38: 100%|██████████| 340/340 [00:13<00:00, 25.65 batch/s, accurracy=1, loss=0.0189]\n",
      "    Validation: 38: 100%|██████████| 76/76 [00:02<00:00, 27.20 batch/s, accurracy=0.9, loss=0.299]\n",
      "          Epoch 39: 100%|██████████| 340/340 [00:13<00:00, 25.26 batch/s, accurracy=1, loss=0.0542]\n",
      "    Validation: 39: 100%|██████████| 76/76 [00:02<00:00, 26.00 batch/s, accurracy=0.86, loss=0.213]\n",
      "          Epoch 40: 100%|██████████| 340/340 [00:13<00:00, 25.22 batch/s, accurracy=1, loss=0.0553]\n",
      "    Validation: 40: 100%|██████████| 76/76 [00:03<00:00, 25.12 batch/s, accurracy=0.78, loss=0.426]\n",
      "          Epoch 41: 100%|██████████| 340/340 [00:13<00:00, 25.00 batch/s, accurracy=1, loss=0.064]\n",
      "    Validation: 41: 100%|██████████| 76/76 [00:02<00:00, 26.98 batch/s, accurracy=0.9, loss=0.192]\n",
      "          Epoch 42: 100%|██████████| 340/340 [00:13<00:00, 25.01 batch/s, accurracy=1, loss=0.0433]\n",
      "    Validation: 42: 100%|██████████| 76/76 [00:02<00:00, 27.93 batch/s, accurracy=0.94, loss=0.127]\n",
      "          Epoch 43: 100%|██████████| 340/340 [00:13<00:00, 24.92 batch/s, accurracy=0.75, loss=0.789]\n",
      "    Validation: 43: 100%|██████████| 76/76 [00:02<00:00, 27.59 batch/s, accurracy=0.82, loss=0.32]\n",
      "          Epoch 44: 100%|██████████| 340/340 [00:13<00:00, 25.21 batch/s, accurracy=0.75, loss=0.518]\n",
      "    Validation: 44: 100%|██████████| 76/76 [00:02<00:00, 26.37 batch/s, accurracy=0.82, loss=0.291]\n",
      "          Epoch 45: 100%|██████████| 340/340 [00:13<00:00, 24.77 batch/s, accurracy=0.75, loss=0.361]\n",
      "    Validation: 45: 100%|██████████| 76/76 [00:02<00:00, 27.78 batch/s, accurracy=0.86, loss=0.279]\n",
      "          Epoch 46: 100%|██████████| 340/340 [00:15<00:00, 22.41 batch/s, accurracy=1, loss=0.0211]\n",
      "    Validation: 46: 100%|██████████| 76/76 [00:02<00:00, 26.13 batch/s, accurracy=0.92, loss=0.204]\n",
      "          Epoch 47: 100%|██████████| 340/340 [00:14<00:00, 23.33 batch/s, accurracy=0.75, loss=0.401]\n",
      "    Validation: 47: 100%|██████████| 76/76 [00:02<00:00, 25.71 batch/s, accurracy=0.96, loss=0.104]\n",
      "          Epoch 48: 100%|██████████| 340/340 [00:13<00:00, 25.33 batch/s, accurracy=1, loss=0.0586]\n",
      "    Validation: 48: 100%|██████████| 76/76 [00:03<00:00, 25.10 batch/s, accurracy=0.9, loss=0.194]\n",
      "          Epoch 49: 100%|██████████| 340/340 [00:13<00:00, 25.73 batch/s, accurracy=1, loss=0.0563]\n",
      "    Validation: 49: 100%|██████████| 76/76 [00:03<00:00, 20.82 batch/s, accurracy=0.88, loss=0.24]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create the loss function weighted to inverse class distribution\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate a Stochastic Gradient Descent optimizer and \"Auto\" Learning Rate schedule.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# Tensorboard writing utility class.\n",
    "writer = SummaryWriter(\"runs/TAK_MLP\")\n",
    "\n",
    "# Perform Training\n",
    "for i in range(EPOCHS):\n",
    "    start_iter = len(train_dataloader) * i\n",
    "    train.train_epoch(i, model, optimizer, loss_function, train_dataloader, start_iter=start_iter, writer=writer)\n",
    "    validation_results = train.evaluate_epoch(i, model, loss_function, val_dataloader, writer)\n",
    "    scheduler.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"model_weights/TAK_MLP_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4120  580]\n",
      " [  24  126]]\n",
      "Precision: 126 / (126 + 580) = 0.1785\n",
      "Recall: 126 / (126 + 24) = 0.84\n",
      "F1: 2 * (0.1785 * 0.84) / (0.1785 + 0.84) = 0.2944\n",
      "AP: 0.15486346777255336\n"
     ]
    }
   ],
   "source": [
    "dev_dataloader = datasets.create_torch_dataloader(val_dataset, vocab,  label_transform, text_transform, weighted=False, batch_size=1)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "labels = []\n",
    "for batch in dev_dataloader:\n",
    "    label, text, *_ = batch\n",
    "    pred_label = train.predict(model, text)\n",
    "    preds.append(pred_label)\n",
    "    labels.append(label.cpu().item())\n",
    "\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(cm)\n",
    "tn, fp, fn, tp = cm.ravel() # Extract the components\n",
    "\n",
    "# Calculate and print Precision\n",
    "precision_string = f\"{tp} / ({tp} + {fp})\"\n",
    "precision = round(eval(precision_string), 4)\n",
    "print(f\"Precision: {precision_string} = {precision}\")\n",
    "\n",
    "# Calculate and print Recall\n",
    "recall_string = f\"{tp} / ({tp} + {fn})\"\n",
    "recall = round(eval(recall_string), 4)\n",
    "print(f\"Recall: {recall_string} = {recall}\")\n",
    "\n",
    "# Calculate and print F1 Score\n",
    "f1_string = f\"2 * ({precision} * {recall}) / ({precision} + {recall})\"\n",
    "f1 = eval(f1_string)\n",
    "print(f\"F1: {f1_string} = {round(f1, 4)}\")\n",
    "\n",
    "\n",
    "print(f\"AP: {average_precision_score(labels, preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions On Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.test.shuf.tsv\", data_columns)\n",
    "test_dataloader = datasets.create_torch_dataloader(test_dataset, vocab, label_transform, text_transform,  weighted=False, batch_size=1)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "doc_ids = []\n",
    "with open(\"RCERESA1.txt\", \"w\") as f:\n",
    "    for batch in test_dataloader:\n",
    "        label, text, offsets, doc_id = batch\n",
    "        pred_label = train.predict(model, text)\n",
    "\n",
    "        pred = pred_label if pred_label == 1 else -1\n",
    "        line = f\"{doc_id[0]}\\t{pred}\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Custom Modules - Source Code**\n",
    "A number of custom code was generated to support this experiment. \n",
    "\n",
    "## ***datasets.py***\n",
    "```python\n",
    "import io\n",
    "from typing import Callable, List\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.utils import unicode_csv_reader\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "_default_tokenizer = get_tokenizer(\"basic_english\")\n",
    "DEFAULT_LABEL_TRANSFORM = lambda x: x\n",
    "DEFAULT_TEXT_TRANSFORM = lambda x: _default_tokenizer(x)\n",
    "\n",
    "\n",
    "def create_torch_dataloader(\n",
    "    dataset: data.Dataset,\n",
    "    vocab: Vocab,\n",
    "    label_transform: Callable = DEFAULT_LABEL_TRANSFORM,\n",
    "    text_transform: Callable = DEFAULT_TEXT_TRANSFORM,\n",
    "    weighted=True,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Creates a Pytorch style dataloader using a dataset and a precompiled vocab.\n",
    "\n",
    "    The dataset returns \"model-ready\" data.\n",
    "\n",
    "    Args:\n",
    "        dataset: The raw text dataset to be used during inference\n",
    "        vocab: the premade vocabulary used to index words/phrases\n",
    "        label_transform: any operation used on the datasets label output\n",
    "        text_transform: operation used on the raw text sentence outputs from the data\n",
    "        weighted: whether to weight the samples based on class distribution\n",
    "        **kwargs: any additional kwargs used by Pytorch DataLoaders.\n",
    "\n",
    "    Returns:\n",
    "        A PyTorch DataLoader to be used during training, eval, or test.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _collate_batch(batch):\n",
    "        label_list, docid_list, text_list, offsets = [], [], [], [0]\n",
    "        for (_label, _docid, _text) in batch:\n",
    "            label_list.append(label_transform(_label))\n",
    "            processed_text = torch.tensor(\n",
    "                vocab(text_transform(_text)), dtype=torch.int64\n",
    "            )\n",
    "            text_list.append(processed_text)\n",
    "            offsets.append(processed_text.size(0))\n",
    "            docid_list.append(_docid)\n",
    "        label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "        text_list = torch.cat(text_list)\n",
    "        return label_list.to(device), text_list.to(device), offsets.to(device), docid_list\n",
    "\n",
    "    if weighted:\n",
    "        weights = dataset.sample_weights\n",
    "        sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights))\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    return data.DataLoader(\n",
    "        dataset,\n",
    "        collate_fn=_collate_batch,\n",
    "        shuffle=(sampler is None),\n",
    "        sampler=sampler,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "class TSVRawTextIterableDataset(data.IterableDataset):\n",
    "    \"\"\"Dataset that loads TSV data incrementally as an iterable and returns raw text.\n",
    "\n",
    "    This dataset must be traversed in order as it only reads from the TSV file as it is called.\n",
    "    Useful if the size of data is too large to load into memory at once.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str, data_columns: List[int]):\n",
    "        \"\"\"Loads an iterator from a file.\n",
    "\n",
    "        Args:\n",
    "            filepath: location of the .tsv file\n",
    "            data_columns: the columns in the .tsv that are used as feature data\n",
    "        \"\"\"\n",
    "        self._number_of_items = _get_tsv_file_length(filepath)\n",
    "        self._iterator = _create_data_from_tsv(\n",
    "            filepath, data_column_indices=data_columns\n",
    "        )\n",
    "        self._current_position = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        item = next(self._iterator)\n",
    "        self._current_position += 1\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._number_of_items\n",
    "\n",
    "\n",
    "class TSVRawTextMapDataset(data.Dataset):\n",
    "    \"\"\"Dataset that loads all TSV data into memory and returns raw text.\n",
    "\n",
    "    This dataset provides a map interface, allowing access to any entry.\n",
    "    Useful for modifying the sampling or order during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str, data_columns: List[int]):\n",
    "        \"\"\"Loads .tsv structed data into memory.\n",
    "\n",
    "        Args:\n",
    "            filepath: location of the .tsv file\n",
    "            data_columns: the columns in the .tsv that are used as feature data\n",
    "        \"\"\"\n",
    "        self._records = list(\n",
    "            _create_data_from_tsv(filepath, data_column_indices=data_columns)\n",
    "        )\n",
    "        self._sample_weights, self._class_weights = self._calculate_weights()\n",
    "\n",
    "    @property\n",
    "    def sample_weights(self):\n",
    "        return self._sample_weights\n",
    "    \n",
    "    @property\n",
    "    def class_weights(self):\n",
    "        return self._class_weights\n",
    "\n",
    "    def _calculate_weights(self):\n",
    "        targets = torch.tensor(\n",
    "            [label if label > 0 else 0 for label, *_ in self._records]\n",
    "        )\n",
    "        unique, sample_counts = torch.unique(targets, return_counts=True)\n",
    "        weight = 1.0 / sample_counts\n",
    "        sample_weights =  torch.tensor([weight[t] for t in targets])\n",
    "        class_weights = weight / weight.sum()\n",
    "        return sample_weights, class_weights\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._records[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._records)\n",
    "\n",
    "\n",
    "def _create_data_from_tsv(data_path, data_column_indices):\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        reader = unicode_csv_reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            data = [row[i] for i in data_column_indices]\n",
    "            yield int(row[0]), row[1], \" \".join(data)\n",
    "\n",
    "\n",
    "def _get_tsv_file_length(data_path):\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        row_count = sum(1 for row in f)\n",
    "\n",
    "    return row_count\n",
    "```\n",
    "\n",
    "## ***models.py***\n",
    "\n",
    "```python\n",
    "import io\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
    "from torchtext.utils import unicode_csv_reader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import vocab as vocab_builder\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "\n",
    "def create_glove_with_unk_vector() -> GloVe:\n",
    "    glove = GloVe()\n",
    "    # Load the average vector for this glove embedding set to use for defaults.\n",
    "    average_glove_vector = np.load(\"../datasets/glove_default_vector.npy\")\n",
    "    unk_init_vec = torch.from_numpy(average_glove_vector)\n",
    "    # Extend the glove vectors with one for \"unk\"\n",
    "    glove.vectors = torch.cat((glove.vectors, unk_init_vec.unsqueeze(0)))\n",
    "\n",
    "    return glove\n",
    "\n",
    "def create_vocab_from_glove(glove: GloVe):\n",
    "    # Since glove is already ordered and not a counter, we overload the\n",
    "    # Constructor to align the indices.\n",
    "    unk_token = \"<unk>\"\n",
    "    vocab = vocab_builder(glove.stoi, min_freq=0)\n",
    "    vocab.append_token(unk_token)\n",
    "    vocab.set_default_index(vocab[unk_token])\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def create_vocab_from_tsv(\n",
    "    filepath: str,\n",
    "    column_indices_to_use: List[int],\n",
    "    minimum_word_freq: int = 1,\n",
    "    ngrams: int = 1,\n",
    "):\n",
    "    \"\"\"Creates a PyTorch vocab object from a TSV file.\n",
    "\n",
    "    The resulting vocab object converts words to indices for assisting in embedding and DL operations.\n",
    "\n",
    "    Args:\n",
    "        filepath: The location of the TSV file\n",
    "        minimum_word_freq: How many times a word must appear to be included\n",
    "        ngrams: The size of ngrams to use for the vocab\n",
    "        column_indices_to_use: Which columns from the TSV are part of the actual feature set\n",
    "\n",
    "    Returns:\n",
    "        A torchtext vocab object.\n",
    "    \"\"\"\n",
    "    unk_token = \"<unk>\"\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        _tsv_iterator(filepath, ngrams=ngrams, column_indices=column_indices_to_use),\n",
    "        min_freq=minimum_word_freq,\n",
    "        specials=[unk_token],\n",
    "    )\n",
    "    vocab.set_default_index(vocab[unk_token])\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def _tsv_iterator(data_path, ngrams, column_indices):\n",
    "    # Spacy has novel tokenizer\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        reader = unicode_csv_reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            row_iter = [row[i] for i in column_indices]\n",
    "            tokens = \" \".join(row_iter)\n",
    "            yield ngrams_iterator(tokenizer(tokens), ngrams)\n",
    "```\n",
    "\n",
    "## ***train.py***\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "from logging import log\n",
    "from typing import Any, Callable, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
    "from torch.utils import data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def predict(model: nn.Module, text: torch.Tensor) -> int:\n",
    "    \"\"\"Predicts the class of a specific text given converted features.\n",
    "    \n",
    "    Args:\n",
    "        model: the model to use for prediction/inferrence\n",
    "        text: the previously converted text (using the prior dictionary)\n",
    "    \n",
    "    Returns:\n",
    "        The predicted label for the provided text.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    no_offset = torch.tensor([0])\n",
    "    with torch.no_grad():\n",
    "        pred_scores = model(text, no_offset)\n",
    "        pred_label = pred_scores.argmax(1).item()\n",
    "        return pred_label\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    epoch_num: int,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_function: Callable,\n",
    "    dataloader: data.DataLoader,\n",
    "    start_iter: int = 0,\n",
    "    log_interval: int = 100,\n",
    "    writer: SummaryWriter = None,\n",
    ") -> int:\n",
    "    \"\"\"Performs training on a single pass through a dataloader.\n",
    "    \n",
    "    Args:\n",
    "        epoch_num: The current number of this epoch of training.    \n",
    "        model: The PyTorch module to train\n",
    "        optimizer: The optimizer to use for training.\n",
    "        loss_function: The function that calculates loss between truth and prediction.\n",
    "        dataloader: Provides a properly formatted batch of data at each iteration.\n",
    "        start_iter: The iteration this epoch started on. Used for plotting.\n",
    "        log_interval: How often to log the scores.\n",
    "        writier: Tensorboard summary writer. \n",
    "    \n",
    "    Returns:\n",
    "        The value of the start_iter plus number of batches performed this epoch.\n",
    "    \"\"\"\n",
    "    batch_counter = start_iter\n",
    "    model.train()\n",
    "    with tqdm(dataloader, unit=\" batch\", bar_format=\"{desc:>20}{percentage:3.0f}%|{bar}{r_bar}\") as tepoch:\n",
    "        for batch in tepoch:\n",
    "            batch_counter += 1\n",
    "            tepoch.set_description(f\"Epoch {epoch_num}\")\n",
    "            results = train_step(batch, model, optimizer, loss_function)\n",
    "            tepoch.set_postfix(loss=results[\"loss\"], accurracy=results[\"accuracy\"])\n",
    "\n",
    "            if writer is not None and batch_counter % log_interval == 0:\n",
    "                writer.add_scalars(\"training\", results, batch_counter)\n",
    "\n",
    "    return batch_counter\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    batch: Tuple[torch.Tensor, ...],\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_function: Callable,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Performs a single training step on a model.\n",
    "\n",
    "    Args:\n",
    "        batch: A previously formatted batch of data.\n",
    "        model: Torch model to perform training on.\n",
    "        optimizer: The optmizer class used in training\n",
    "        loss_function: The callable function to generate loss between prediction and truth.\n",
    "\n",
    "    Returns:\n",
    "        The different metrics generated this training step.\n",
    "    \"\"\"\n",
    "    labels, text, offsets, *_ = batch\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    predicted_scores = model(text, offsets)\n",
    "    loss = loss_function(predicted_scores, labels)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "    optimizer.step()\n",
    "\n",
    "    predicted_labels = predicted_scores.argmax(1)\n",
    "\n",
    "    accuracy = (predicted_labels == labels).sum().item() / labels.size(0)\n",
    "\n",
    "    y_true = labels.detach().cpu().numpy()\n",
    "    y_pred = predicted_labels.cpu().numpy()\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        average=\"binary\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"loss\": loss.detach().cpu().item(),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"fscore\": fscore,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_epoch(\n",
    "    epoch_num: int,\n",
    "    model: nn.Module,\n",
    "    loss_function: Callable,\n",
    "    dataloader: data.DataLoader,\n",
    "    writer: SummaryWriter = None,\n",
    ")-> Dict[str, float]:\n",
    "    \"\"\"Performs validation on a single pass through a dataloader.\n",
    "    \n",
    "    Args:\n",
    "        epoch_num: The current number of this epoch of training.    \n",
    "        model: The PyTorch module to train\n",
    "        loss_function: The function that calculates loss between truth and prediction.\n",
    "        dataloader: Provides a properly formatted batch of data at each iteration.\n",
    "        writier: Tensorboard summary writer. \n",
    "    \n",
    "    Returns:\n",
    "        The average validation metrics for the whole dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    aggregate_results = Counter()\n",
    "    with tqdm(dataloader, unit=\" batch\", bar_format=\"{desc:>20}{percentage:3.0f}%|{bar}{r_bar}\") as tepoch:\n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Validation: {epoch_num}\")\n",
    "            results = evaluate_step(batch, model, loss_function)\n",
    "            tepoch.set_postfix(loss=results[\"loss\"], accurracy=results[\"accuracy\"])\n",
    "            aggregate_results += Counter(results)\n",
    "\n",
    "        average_results = {\n",
    "            key: aggregate_results[key] / tepoch.total for key in aggregate_results\n",
    "        }\n",
    "\n",
    "    writer.add_scalars(\"validation\", average_results, epoch_num)\n",
    "    return average_results\n",
    "\n",
    "\n",
    "def evaluate_step(\n",
    "    batch: Tuple[torch.Tensor, ...],\n",
    "    model: nn.Module,\n",
    "    loss_function: Callable,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Performs a single validation step on a model.\n",
    "\n",
    "    Args:\n",
    "        batch: A previously formatted batch of data.\n",
    "        model: Torch model to perform training on.\n",
    "        loss_function: The callable function to generate loss between prediction and truth.\n",
    "\n",
    "    Returns:\n",
    "        The different metrics generated this training step.\n",
    "    \"\"\"\n",
    "\n",
    "    labels, text, offsets, *_ = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_scores = model(text, offsets)\n",
    "        loss = loss_function(predicted_scores, labels)\n",
    "        predicted_labels = predicted_scores.argmax(1)\n",
    "        accuracy = (predicted_labels == labels).sum().item() / labels.size(0)\n",
    "\n",
    "        y_true = labels.detach().cpu().numpy()\n",
    "        y_pred = predicted_labels.cpu().numpy()\n",
    "\n",
    "        precision, recall, fscore, support = precision_recall_fscore_support(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=\"binary\",\n",
    "            zero_division=0,\n",
    "        )\n",
    "        \n",
    "    results = {\n",
    "        \"loss\": loss.detach().cpu().item(),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"fscore\": fscore,\n",
    "    }\n",
    "    return results\n",
    "```\n",
    "\n",
    "## ***vocab.py***\n",
    "\n",
    "```python\n",
    "import io\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
    "from torchtext.utils import unicode_csv_reader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import vocab as vocab_builder\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "\n",
    "def create_glove_with_unk_vector() -> GloVe:\n",
    "    glove = GloVe()\n",
    "    # Load the average vector for this glove embedding set to use for defaults.\n",
    "    average_glove_vector = np.load(\"../datasets/glove_default_vector.npy\")\n",
    "    unk_init_vec = torch.from_numpy(average_glove_vector)\n",
    "    # Extend the glove vectors with one for \"unk\"\n",
    "    glove.vectors = torch.cat((glove.vectors, unk_init_vec.unsqueeze(0)))\n",
    "\n",
    "    return glove\n",
    "\n",
    "def create_vocab_from_glove(glove: GloVe):\n",
    "    # Since glove is already ordered and not a counter, we overload the\n",
    "    # Constructor to align the indices.\n",
    "    unk_token = \"<unk>\"\n",
    "    vocab = vocab_builder(glove.stoi, min_freq=0)\n",
    "    vocab.append_token(unk_token)\n",
    "    vocab.set_default_index(vocab[unk_token])\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def create_vocab_from_tsv(\n",
    "    filepath: str,\n",
    "    column_indices_to_use: List[int],\n",
    "    minimum_word_freq: int = 1,\n",
    "    ngrams: int = 1,\n",
    "):\n",
    "    \"\"\"Creates a PyTorch vocab object from a TSV file.\n",
    "\n",
    "    The resulting vocab object converts words to indices for assisting in embedding and DL operations.\n",
    "\n",
    "    Args:\n",
    "        filepath: The location of the TSV file\n",
    "        minimum_word_freq: How many times a word must appear to be included\n",
    "        ngrams: The size of ngrams to use for the vocab\n",
    "        column_indices_to_use: Which columns from the TSV are part of the actual feature set\n",
    "\n",
    "    Returns:\n",
    "        A torchtext vocab object.\n",
    "    \"\"\"\n",
    "    unk_token = \"<unk>\"\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        _tsv_iterator(filepath, ngrams=ngrams, column_indices=column_indices_to_use),\n",
    "        min_freq=minimum_word_freq,\n",
    "        specials=[unk_token],\n",
    "    )\n",
    "    vocab.set_default_index(vocab[unk_token])\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def _tsv_iterator(data_path, ngrams, column_indices):\n",
    "    # Spacy has novel tokenizer\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        reader = unicode_csv_reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            row_iter = [row[i] for i in column_indices]\n",
    "            tokens = \" \".join(row_iter)\n",
    "            yield ngrams_iterator(tokenizer(tokens), ngrams)\n",
    "\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2c370576f24a547f2bd7f1badbf1e39ac1d010b680ddb4523878c870f4696a1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ir-classification-_Pgcz6ju-py3.9': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
