{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #4\n",
    "\n",
    "**Authors:** *Ryan Ceresani*\n",
    "\n",
    "**Class:** *605.744 Information Retrieval*\n",
    "\n",
    "**Term:** *Fall 2021*\n",
    "\n",
    "This assignment submission is structured slightly different than my previous - as we are using a Notebook in place of a \"main\" application. \n",
    "The notebook provides a nice integration for intermediate code output, plotting, results and experimentation.\n",
    "\n",
    "## Expected Deliverables\n",
    "- Report outlining methodologies, tools used, parameter decisions, etc.\n",
    "- Precision, Recall, F1 score for the `\"dev\"` dataset using \"Title\" column as features.\n",
    "- Metrics for `\"dev\"` dataset using \"Title\", \"Abstract\", and \"Keywords\" as features.\n",
    "- Perform an additional non-trival experimentation.\n",
    "- Predictions for the `\"test\"` dataset.\n",
    "\n",
    "\n",
    "## **Overall Methodology**\n",
    "Before getting into code specifics, we will address some overall elements used during this assignment. The main approach I am choosing to perform **Text Classification** is through deep learning.\n",
    "\n",
    "### Open Source Libraries\n",
    "To facility deep learning, this experiment makes heavy use of `PyTorch` and `torchtext` for doing the underlying operations.  Additionally, `scikit-learn` is useful for their `metrics` library which offers useful calculation ability for a variety of classifation metrics (whether you use one of their estimators or not.)  \n",
    "- **PyTorch**: This is the heavy-lifter for a number of backing abstract classes performing a variety of functions.\n",
    "    - `data`: We created custom PyTorch `Dataset` and `DataLoader` classes for batching, sampling, and shuffling our training, validation, and test data as appropriate.\n",
    "    - `nn.Module`: This is the abstract base class for most things in `PyTorch` but is the foundation for any model or classifier we will use.\n",
    "    - `optim`: This provides an optimizer and learning rate scheduler for the training loops. One of the powerful utilities of PyTorch is the way it hides gradient interactions and backward propagation from a user.\n",
    "    - `loss`: Combined with `optim` the loss module provides the ability to generate loss from our predictions. Specifically we use weighted CrossEntropyLoss.\n",
    "    - **General PyTorch magic:** PyTorch also provides some other nice things when it comes to the gradient operations being attached to tensors or the built-in CUDA support. \n",
    "-  **Torchtext**: Extension off the official PyTorch to provide text based utilities.\n",
    "   -  `vocab`: This generates a `Vocabulary` object from an iterator which can be used to map words to indices, converting tokens into values.\n",
    "   -  `utils`: It also has some built in utilities for tokenizing and creating ngrams.\n",
    "- **scikit-learn**: Used for the `metrics` library to generate scores and repots.\n",
    "\n",
    "### **Dataset**\n",
    "Systematic Review\n",
    "\n",
    "- NOTE: The Document **hash:a8113f0b-6561-3178-8c2d-7b4ebac229ff** contained an odd sequence of characters in UTF8 at the beginning of the \"Article\" section which caused problems for the python stdlib `csv_reader`. The value was sanitized to remove the characters (`\",`) - which would be removed in tokenization anyway. \n",
    "\n",
    "#### Dataset Challenges\n",
    "The primary challenge imposed by this dataset is the vast class imbalance within training. (30:1 negative skew)\n",
    "\n",
    "To counteract this, two approaches were used in tandem:\n",
    "1. The chosen loss function (`CrossEntropyLoss`) used inverse class frequency weighting to encourage learning on the minority class.\n",
    "2. The `DataLoader` used a `WeightedRandomSampler` which was weighted to essentially up-sample the minority class and create an artificial balance.\n",
    "\n",
    "### **Major Parameters**\n",
    "Across all of the libraries and custom code (to be shown below) there are a number of key parameters that influence the results. They will be broken down into categories.\n",
    "\n",
    "- Data\n",
    "  - `batch_size`: Batch size when processing data can impact the quality of training depending on application.\n",
    "    - **A batch size of 32 was used to balance between training speed and overgeneralization.**\n",
    "  - `data_columns`: This specifies which columns of the *.tsv* file are to be used as features. \n",
    "    - **The prompt specifies we use first *text* and then *text + abstract + keyword***\n",
    "  - `ngrams`: how large the ngram value should be for the dataset. \n",
    "    - **Initial value set to `unigram` or 1.**\n",
    "  - `tokenizer`: `torchtext` comes with a number of pre-made tokenizers, each having slight variations. \n",
    "    - **For the initial pass we will use `basic_english`**. \n",
    "  - `weighted`: Whether or not the dataset should be weighted to oversample minority classes.\n",
    "    - **With this dataset, it greatly enhances training so it is turned on.**\n",
    "  \n",
    "\n",
    "\n",
    "- Model\n",
    "  - The model itself counts as a parameter and in this case it is as a very simple architecture with an `EmbeddingBag` and a fully connected linear layer.\n",
    "    - The hyperparameters for the model are:\n",
    "      - `vocab_size`: Which is directly tied to the dataset and not exactly a parameter.\n",
    "      - `embedding_dim`: The size of the embedding and consequently the input to the linear layer.\n",
    "        - **Initially set to 64**\n",
    "  \n",
    "- Training\n",
    "  - `epochs`: How many times through the dataset to train. Depends on time/resources available. \n",
    "    - **Starting with 25 epochs to evaluate training results and will adjust from there.**\n",
    "  - `learning_rate`: The learning rate directly impacts the optimizer, but due to the decoupled nature, can be changed fluidly.\n",
    "    - **It is starting at a very high value of 5 to encourage faster convergence.**\n",
    "  - `learning_rate_scheduler`: The methodology for updating the learning rate. We will be reducing the learning rate during a plateau of metric values.\n",
    "    - **Have chosen to reduce LR when plateuing on the F1 score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Custom Modules - Source Code**\n",
    "A number of custom code was generated to support this experiment. \n",
    "\n",
    "### datasets.py\n",
    "```python\n",
    "import io\n",
    "from typing import Callable, List\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.utils import unicode_csv_reader\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "_default_tokenizer = get_tokenizer(\"basic_english\")\n",
    "DEFAULT_LABEL_TRANSFORM = lambda x: x\n",
    "DEFAULT_TEXT_TRANSFORM = lambda x: _default_tokenizer(x)\n",
    "\n",
    "\n",
    "def create_torch_dataloader(\n",
    "    dataset: data.Dataset,\n",
    "    vocab: Vocab,\n",
    "    label_transform: Callable = DEFAULT_LABEL_TRANSFORM,\n",
    "    text_transform: Callable = DEFAULT_TEXT_TRANSFORM,\n",
    "    weighted=True,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Creates a Pytorch style dataloader using a dataset and a precompiled vocab.\n",
    "\n",
    "    The dataset returns \"model-ready\" data.\n",
    "\n",
    "    Args:\n",
    "        dataset: The raw text dataset to be used during inference\n",
    "        vocab: the premade vocabulary used to index words/phrases\n",
    "        label_transform: any operation used on the datasets label output\n",
    "        text_transform: operation used on the raw text sentence outputs from the data\n",
    "        weighted: whether to weight the samples based on class distribution\n",
    "        **kwargs: any additional kwargs used by Pytorch DataLoaders.\n",
    "\n",
    "    Returns:\n",
    "        A PyTorch DataLoader to be used during training, eval, or test.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _collate_batch(batch):\n",
    "        label_list, docid_list, text_list, offsets = [], [], [], [0]\n",
    "        for (_label, _docid, _text) in batch:\n",
    "            label_list.append(label_transform(_label))\n",
    "            processed_text = torch.tensor(\n",
    "                vocab(text_transform(_text)), dtype=torch.int64\n",
    "            )\n",
    "            text_list.append(processed_text)\n",
    "            offsets.append(processed_text.size(0))\n",
    "            docid_list.append(_docid)\n",
    "        label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "        text_list = torch.cat(text_list)\n",
    "        return label_list.to(device), text_list.to(device), offsets.to(device), docid_list\n",
    "\n",
    "    if weighted:\n",
    "        weights = dataset.sample_weights\n",
    "        sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights))\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    return data.DataLoader(\n",
    "        dataset,\n",
    "        collate_fn=_collate_batch,\n",
    "        shuffle=(sampler is None),\n",
    "        sampler=sampler,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "class TSVRawTextIterableDataset(data.IterableDataset):\n",
    "    \"\"\"Dataset that loads TSV data incrementally as an iterable and returns raw text.\n",
    "\n",
    "    This dataset must be traversed in order as it only reads from the TSV file as it is called.\n",
    "    Useful if the size of data is too large to load into memory at once.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str, data_columns: List[int]):\n",
    "        \"\"\"Loads an iterator from a file.\n",
    "\n",
    "        Args:\n",
    "            filepath: location of the .tsv file\n",
    "            data_columns: the columns in the .tsv that are used as feature data\n",
    "        \"\"\"\n",
    "        self._number_of_items = _get_tsv_file_length(filepath)\n",
    "        self._iterator = _create_data_from_tsv(\n",
    "            filepath, data_column_indices=data_columns\n",
    "        )\n",
    "        self._current_position = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        item = next(self._iterator)\n",
    "        self._current_position += 1\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._number_of_items\n",
    "\n",
    "\n",
    "class TSVRawTextMapDataset(data.Dataset):\n",
    "    \"\"\"Dataset that loads all TSV data into memory and returns raw text.\n",
    "\n",
    "    This dataset provides a map interface, allowing access to any entry.\n",
    "    Useful for modifying the sampling or order during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str, data_columns: List[int]):\n",
    "        \"\"\"Loads .tsv structed data into memory.\n",
    "\n",
    "        Args:\n",
    "            filepath: location of the .tsv file\n",
    "            data_columns: the columns in the .tsv that are used as feature data\n",
    "        \"\"\"\n",
    "        self._records = list(\n",
    "            _create_data_from_tsv(filepath, data_column_indices=data_columns)\n",
    "        )\n",
    "        self._sample_weights, self._class_weights = self._calculate_weights()\n",
    "\n",
    "    @property\n",
    "    def sample_weights(self):\n",
    "        return self._sample_weights\n",
    "    \n",
    "    @property\n",
    "    def class_weights(self):\n",
    "        return self._class_weights\n",
    "\n",
    "    def _calculate_weights(self):\n",
    "        targets = torch.tensor(\n",
    "            [label if label > 0 else 0 for label, *_ in self._records]\n",
    "        )\n",
    "        unique, sample_counts = torch.unique(targets, return_counts=True)\n",
    "        weight = 1.0 / sample_counts\n",
    "        sample_weights =  torch.tensor([weight[t] for t in targets])\n",
    "        class_weights = weight / weight.sum()\n",
    "        return sample_weights, class_weights\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._records[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._records)\n",
    "\n",
    "\n",
    "def _create_data_from_tsv(data_path, data_column_indices):\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        reader = unicode_csv_reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            data = [row[i] for i in data_column_indices]\n",
    "            yield int(row[0]), row[1], \" \".join(data)\n",
    "\n",
    "\n",
    "def _get_tsv_file_length(data_path):\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        row_count = sum(1 for row in f)\n",
    "\n",
    "    return row_count\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Walkthrough\n",
    "\n",
    "We will now walk through the experiment notebook to see results in action.\n",
    "\n",
    "### Imports\n",
    "\n",
    "The open source and custom modules used are imported first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ir_classification import datasets, models\n",
    "from ir_classification import vocab as ir_vocab\n",
    "from ir_classification import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafield_map = {\"assessment\": 0, \"doc_id\": 1, \"title\": 2, \"authors\": 3, \"journal\": 4, \"issn\": 5, \"year\": 6, \"language\": 7, \"abstract\": 8, \"keywords\": 9}\n",
    "data_columns = [datafield_map[\"title\"]]\n",
    "ngrams = 1\n",
    "batch_size = 32\n",
    "\n",
    "# Create vocab from the training data.\n",
    "vocab = ir_vocab.create_vocab_from_tsv(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns, ngrams=ngrams)\n",
    "\n",
    "# Load the TSV into datasets with the appropriate feature columns.\n",
    "train_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns)\n",
    "val_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.dev.shuf.tsv\", data_columns)\n",
    "\n",
    "# Create the transforms for the dataloader to appropriately format the contents of the files.\n",
    "label_transform = lambda x: x if x > 0 else 0\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "text_transform = lambda x: list(ngrams_iterator(tokenizer(x), ngrams))\n",
    "\n",
    "# Instantiate the dataloaders.\n",
    "train_dataloader = datasets.create_torch_dataloader(train_dataset, vocab,  label_transform, text_transform, weighted=True, batch_size=batch_size)\n",
    "val_dataloader = datasets.create_torch_dataloader(val_dataset, vocab,  label_transform, text_transform, weighted=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "vocab_size = len(vocab) # from vocab created earlier.\n",
    "embedding_size = 64\n",
    "\n",
    "# Enable compatability when training with GPU enabled devices.  \n",
    "# (Some development work was done in Google Colab with GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.EmbeddingBagLinearModel(vocab_size, embedding_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the top-level training loop\n",
    "\n",
    "The custom code was meant to handle the individual `step` and `epoch` levels generically.\n",
    "This setup should let us change the components experimentally in cells like below without much other hassle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      Epoch 0: 100%|██████████| 677/677 [00:04<00:00, 137.93 batch/s, accurracy=0.533, loss=0.155]\n",
      "Validation: 0: 100%|██████████| 152/152 [00:00<00:00, 181.87 batch/s, accurracy=0.444, loss=0.585]\n",
      "      Epoch 1: 100%|██████████| 677/677 [00:04<00:00, 137.02 batch/s, accurracy=0.933, loss=0.0241]\n",
      "Validation: 1: 100%|██████████| 152/152 [00:00<00:00, 217.25 batch/s, accurracy=0.611, loss=1.3]\n",
      "      Epoch 2: 100%|██████████| 677/677 [00:04<00:00, 151.99 batch/s, accurracy=0.867, loss=0.0287]\n",
      "Validation: 2: 100%|██████████| 152/152 [00:00<00:00, 214.50 batch/s, accurracy=0.778, loss=1.16]\n",
      "      Epoch 3: 100%|██████████| 677/677 [00:04<00:00, 165.61 batch/s, accurracy=0.967, loss=0.0217]\n",
      "Validation: 3: 100%|██████████| 152/152 [00:00<00:00, 241.91 batch/s, accurracy=0.889, loss=0.175]\n",
      "      Epoch 4: 100%|██████████| 677/677 [00:04<00:00, 167.02 batch/s, accurracy=0.967, loss=0.00881]\n",
      "Validation: 4: 100%|██████████| 152/152 [00:00<00:00, 249.64 batch/s, accurracy=0.889, loss=0.113]\n",
      "      Epoch 5: 100%|██████████| 677/677 [00:04<00:00, 166.73 batch/s, accurracy=1, loss=0.00368]\n",
      "Validation: 5: 100%|██████████| 152/152 [00:00<00:00, 241.56 batch/s, accurracy=0.944, loss=0.207]\n",
      "      Epoch 6: 100%|██████████| 677/677 [00:03<00:00, 175.95 batch/s, accurracy=0.933, loss=0.0196]\n",
      "Validation: 6: 100%|██████████| 152/152 [00:00<00:00, 256.15 batch/s, accurracy=0.944, loss=0.453]\n",
      "      Epoch 7: 100%|██████████| 677/677 [00:04<00:00, 153.02 batch/s, accurracy=0.967, loss=0.00721]\n",
      "Validation: 7: 100%|██████████| 152/152 [00:00<00:00, 214.05 batch/s, accurracy=0.833, loss=0.181]\n",
      "      Epoch 8: 100%|██████████| 677/677 [00:04<00:00, 153.76 batch/s, accurracy=0.967, loss=0.013]\n",
      "Validation: 8: 100%|██████████| 152/152 [00:00<00:00, 208.00 batch/s, accurracy=0.944, loss=0.187]\n",
      "      Epoch 9: 100%|██████████| 677/677 [00:04<00:00, 150.00 batch/s, accurracy=1, loss=0.0157]\n",
      "Validation: 9: 100%|██████████| 152/152 [00:00<00:00, 223.66 batch/s, accurracy=1, loss=0.0996]\n",
      "     Epoch 10: 100%|██████████| 677/677 [00:04<00:00, 150.99 batch/s, accurracy=0.933, loss=0.00893]\n",
      "Validation: 10: 100%|██████████| 152/152 [00:00<00:00, 198.18 batch/s, accurracy=0.944, loss=0.549]\n",
      "     Epoch 11: 100%|██████████| 677/677 [00:04<00:00, 152.83 batch/s, accurracy=0.933, loss=0.0173]\n",
      "Validation: 11: 100%|██████████| 152/152 [00:00<00:00, 210.23 batch/s, accurracy=0.944, loss=0.185]\n",
      "     Epoch 12: 100%|██████████| 677/677 [00:03<00:00, 170.15 batch/s, accurracy=1, loss=0.00209]\n",
      "Validation: 12: 100%|██████████| 152/152 [00:00<00:00, 235.56 batch/s, accurracy=0.944, loss=0.127]\n",
      "     Epoch 13: 100%|██████████| 677/677 [00:03<00:00, 174.30 batch/s, accurracy=0.967, loss=0.00613]\n",
      "Validation: 13: 100%|██████████| 152/152 [00:00<00:00, 219.46 batch/s, accurracy=0.944, loss=0.0479]\n",
      "     Epoch 14: 100%|██████████| 677/677 [00:04<00:00, 160.29 batch/s, accurracy=1, loss=0.00311]\n",
      "Validation: 14: 100%|██████████| 152/152 [00:00<00:00, 210.52 batch/s, accurracy=0.833, loss=0.817]\n",
      "     Epoch 15: 100%|██████████| 677/677 [00:04<00:00, 159.48 batch/s, accurracy=1, loss=0.0021]\n",
      "Validation: 15: 100%|██████████| 152/152 [00:00<00:00, 235.51 batch/s, accurracy=0.944, loss=0.341]\n",
      "     Epoch 16: 100%|██████████| 677/677 [00:04<00:00, 155.38 batch/s, accurracy=1, loss=0.00125]\n",
      "Validation: 16: 100%|██████████| 152/152 [00:00<00:00, 212.36 batch/s, accurracy=0.944, loss=0.196]\n",
      "     Epoch 17: 100%|██████████| 677/677 [00:04<00:00, 148.53 batch/s, accurracy=0.967, loss=0.00321]\n",
      "Validation: 17: 100%|██████████| 152/152 [00:00<00:00, 223.14 batch/s, accurracy=0.944, loss=0.116]\n",
      "     Epoch 18: 100%|██████████| 677/677 [00:04<00:00, 149.52 batch/s, accurracy=0.933, loss=0.00716]\n",
      "Validation: 18: 100%|██████████| 152/152 [00:00<00:00, 220.40 batch/s, accurracy=1, loss=0.0191]\n",
      "     Epoch 19: 100%|██████████| 677/677 [00:04<00:00, 147.77 batch/s, accurracy=0.933, loss=0.039]\n",
      "Validation: 19: 100%|██████████| 152/152 [00:00<00:00, 213.16 batch/s, accurracy=0.944, loss=0.75]\n",
      "     Epoch 20: 100%|██████████| 677/677 [00:04<00:00, 162.26 batch/s, accurracy=0.967, loss=0.00352]\n",
      "Validation: 20: 100%|██████████| 152/152 [00:00<00:00, 224.45 batch/s, accurracy=1, loss=0.000391]\n",
      "     Epoch 21: 100%|██████████| 677/677 [00:04<00:00, 157.89 batch/s, accurracy=1, loss=0.00113]\n",
      "Validation: 21: 100%|██████████| 152/152 [00:00<00:00, 231.82 batch/s, accurracy=0.944, loss=0.604]\n",
      "     Epoch 22: 100%|██████████| 677/677 [00:04<00:00, 153.66 batch/s, accurracy=1, loss=0.000955]\n",
      "Validation: 22: 100%|██████████| 152/152 [00:00<00:00, 192.79 batch/s, accurracy=0.944, loss=16.4]\n",
      "     Epoch 23: 100%|██████████| 677/677 [00:04<00:00, 142.98 batch/s, accurracy=1, loss=0.00158]\n",
      "Validation: 23: 100%|██████████| 152/152 [00:00<00:00, 206.93 batch/s, accurracy=1, loss=0.0495]\n",
      "     Epoch 24: 100%|██████████| 677/677 [00:04<00:00, 147.62 batch/s, accurracy=1, loss=0.00139]\n",
      "Validation: 24: 100%|██████████| 152/152 [00:00<00:00, 203.72 batch/s, accurracy=1, loss=0.0032]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "learning_rate = 5 \n",
    "\n",
    "# Create the loss function weighted to inverse class distribution\n",
    "loss_function = torch.nn.CrossEntropyLoss(weight=train_dataset.class_weights)\n",
    "\n",
    "# Instantiate a Stochastic Gradient Descent optimizer and \"Auto\" Learning Rate schedule.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"max\")\n",
    "\n",
    "# Tensorboard writing utility class.\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Perform Training\n",
    "for i in range(EPOCHS):\n",
    "    start_iter = len(train_dataloader) * i\n",
    "    train.train_epoch(i, model, optimizer, loss_function, train_dataloader, start_iter=start_iter, writer=writer)\n",
    "    validation_results = train.evaluate_epoch(i, model, loss_function, val_dataloader, writer)\n",
    "    scheduler.step(validation_results[\"fscore\"])\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"model_weights/title_only_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Metrics on the Dev Set\n",
    "Here we recreate the `dev` dataloader to go a single document at a time.\n",
    "\n",
    "We use the less robust `predict` method so we can explicitly show the values and calculations being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataloader = datasets.create_torch_dataloader(val_dataset, vocab,  label_transform, text_transform, weighted=False, batch_size=1)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "labels = []\n",
    "for batch in dev_dataloader:\n",
    "    label, text, *_ = batch\n",
    "    pred_label = train.predict(model, text)\n",
    "    preds.append(pred_label)\n",
    "    labels.append(label.cpu().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Confusion Matrix\n",
    "\n",
    "We can use the confusion matrix to make it easy to visualize the values for the metrics.  \n",
    "       \n",
    "\n",
    "       \n",
    "|   | P0  | P1  |\n",
    "|---|----|----|\n",
    "| A0 | TN | FP |\n",
    "| A1 | FN | TP |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4387  313]\n",
      " [  68   82]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Precision, Recall, F1-Score on Dev Set\n",
    "We use a confusion matrix to make it easy to map out the values for true positive, true negative, false positive, and false negative.\n",
    "\n",
    "- Precision = tp / (tp + fp)\n",
    "- Recall = tp / (tp + fn)\n",
    "- F1 Score =  2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 82 / (82 + 313) = 0.2076\n",
      "Recall: 82 / (82 + 68) = 0.5467\n",
      "F1: 2 * (0.2076 * 0.5467) / (0.2076 + 0.5467) = 0.3009\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = cm.ravel() # Extract the components\n",
    "\n",
    "# Calculate and print Precision\n",
    "precision_string = f\"{tp} / ({tp} + {fp})\"\n",
    "precision = round(eval(precision_string), 4)\n",
    "print(f\"Precision: {precision_string} = {precision}\")\n",
    "\n",
    "# Calculate and print Recall\n",
    "recall_string = f\"{tp} / ({tp} + {fn})\"\n",
    "recall = round(eval(recall_string), 4)\n",
    "print(f\"Recall: {recall_string} = {recall}\")\n",
    "\n",
    "# Calculate and print F1 Score\n",
    "f1_string = f\"2 * ({precision} * {recall}) / ({precision} + {recall})\"\n",
    "f1 = eval(f1_string)\n",
    "print(f\"F1: {f1_string} = {round(f1, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat Experiment with \"Title\", \"Abstract\", and \"Keyword\" Data\n",
    "\n",
    "We will keep everything exactly the same for setup, changing only the things needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_columns = [\"title\", \"abstract\", \"keywords\"]\n",
    "data_columns = [datafield_map[col] for col in use_columns]\n",
    "ngrams = 1\n",
    "\n",
    "# Create vocab from the training data.\n",
    "vocab = ir_vocab.create_vocab_from_tsv(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns, ngrams=ngrams)\n",
    "\n",
    "# Load the TSV into datasets with the appropriate feature columns.\n",
    "train_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns)\n",
    "val_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.dev.shuf.tsv\", data_columns)\n",
    "\n",
    "# Instantiate the dataloaders.\n",
    "train_dataloader = datasets.create_torch_dataloader(train_dataset, vocab,  label_transform, text_transform, weighted=True, batch_size=batch_size)\n",
    "val_dataloader = datasets.create_torch_dataloader(val_dataset, vocab,  label_transform, text_transform, weighted=False, batch_size=batch_size)\n",
    "\n",
    "# Create Model\n",
    "vocab_size = len(vocab)\n",
    "model = models.EmbeddingBagLinearModel(vocab_size, embedding_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      Epoch 0:  51%|█████▏    | 349/679 [00:13<00:12, 27.37 batch/s, accurracy=0.5, loss=0.125]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "learning_rate = 5 \n",
    "\n",
    "# Create the loss function weighted to inverse class distribution\n",
    "loss_function = torch.nn.CrossEntropyLoss(weight=train_dataset.class_weights)\n",
    "\n",
    "# Instantiate a Stochastic Gradient Descent optimizer and \"Auto\" Learning Rate schedule.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"max\")\n",
    "\n",
    "# Tensorboard writing utility class.\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Perform Training\n",
    "for i in range(EPOCHS):\n",
    "    start_iter = len(train_dataloader) * i\n",
    "    train.train_epoch(i, model, optimizer, loss_function, train_dataloader, start_iter=start_iter, writer=writer)\n",
    "    validation_results = train.evaluate_epoch(i, model, loss_function, val_dataloader, writer)\n",
    "    scheduler.step(validation_results[\"fscore\"])\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"model_weights/tak_state_dict.pth\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2c370576f24a547f2bd7f1badbf1e39ac1d010b680ddb4523878c870f4696a1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ir-classification-_Pgcz6ju-py3.9': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
