{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #4\n",
    "\n",
    "**Authors:** *Ryan Ceresani*\n",
    "\n",
    "**Class:** *605.744 Information Retrieval*\n",
    "\n",
    "**Term:** *Fall 2021*\n",
    "\n",
    "This assignment submission is structured slightly different than my previous - as we are using a Notebook in place of a \"main\" application. \n",
    "The notebook provides a nice integration for intermediate code output, plotting, results and experimentation.\n",
    "\n",
    "## Expected Deliverables\n",
    "- Report outlining methodologies, tools used, parameter decisions, etc.\n",
    "- Precision, Recall, F1 score for the `\"dev\"` dataset using \"Title\" column as features.\n",
    "- Metrics for `\"dev\"` dataset using \"Title\", \"Abstract\", and \"Keywords\" as features.\n",
    "- Perform an additional non-trival experimentation.\n",
    "- Predictions for the `\"test\"` dataset.\n",
    "\n",
    "\n",
    "## **Overall Methodology**\n",
    "Before getting into code specifics, we will address some overall elements used during this assignment. The main approach I am choosing to perform **Text Classification** is through deep learning.\n",
    "\n",
    "### Open Source Libraries\n",
    "To facility deep learning, this experiment makes heavy use of `PyTorch` and `torchtext` for doing the underlying operations.  Additionally, `scikit-learn` is useful for their `metrics` library which offers useful calculation ability for a variety of classifation metrics (whether you use one of their estimators or not.)  \n",
    "- **PyTorch**: This is the heavy-lifter for a number of backing abstract classes performing a variety of functions.\n",
    "    - `data`: We created custom PyTorch `Dataset` and `DataLoader` classes for batching, sampling, and shuffling our training, validation, and test data as appropriate.\n",
    "    - `nn.Module`: This is the abstract base class for most things in `PyTorch` but is the foundation for any model or classifier we will use.\n",
    "    - `optim`: This provides an optimizer and learning rate scheduler for the training loops. One of the powerful utilities of PyTorch is the way it hides gradient interactions and backward propagation from a user.\n",
    "    - `loss`: Combined with `optim` the loss module provides the ability to generate loss from our predictions. Specifically we use weighted CrossEntropyLoss.\n",
    "    - **General PyTorch magic:** PyTorch also provides some other nice things when it comes to the gradient operations being attached to tensors or the built-in CUDA support. \n",
    "-  **Torchtext**: Extension off the official PyTorch to provide text based utilities.\n",
    "   -  `vocab`: This generates a `Vocabulary` object from an iterator which can be used to map words to indices, converting tokens into values.\n",
    "   -  `utils`: It also has some built in utilities for tokenizing and creating ngrams.\n",
    "- **scikit-learn**: Used for the `metrics` library to generate scores and repots.\n",
    "\n",
    "### **Dataset**\n",
    "Systematic Review\n",
    "\n",
    "- NOTE: The Document **hash:a8113f0b-6561-3178-8c2d-7b4ebac229ff** contained an odd sequence of characters in UTF8 at the beginning of the \"Article\" section which caused problems for the python stdlib `csv_reader`. The value was sanitized to remove the characters (`\",`) - which would be removed in tokenization anyway. \n",
    "\n",
    "#### Dataset Challenges\n",
    "The primary challenge imposed by this dataset is the vast class imbalance within training. (30:1 negative skew)\n",
    "\n",
    "To counteract this, two approaches were used in tandem:\n",
    "1. The chosen loss function (`CrossEntropyLoss`) used inverse class frequency weighting to encourage learning on the minority class.\n",
    "2. The `DataLoader` used a `WeightedRandomSampler` which was weighted to essentially up-sample the minority class and create an artificial balance.\n",
    "\n",
    "### **Major Parameters**\n",
    "Across all of the libraries and custom code (to be shown below) there are a number of key parameters that influence the results. They will be broken down into categories.\n",
    "\n",
    "- Data\n",
    "  - `batch_size`: Batch size when processing data can impact the quality of training depending on application.\n",
    "    - **A batch size of 32 was used to balance between training speed and overgeneralization.**\n",
    "  - `data_columns`: This specifies which columns of the *.tsv* file are to be used as features. \n",
    "    - **The prompt specifies we use first *text* and then *text + abstract + keyword***\n",
    "  - `ngrams`: how large the ngram value should be for the dataset. \n",
    "    - **Initial value set to `unigram` or 1.**\n",
    "  - `tokenizer`: `torchtext` comes with a number of pre-made tokenizers, each having slight variations. \n",
    "    - **For the initial pass we will use `basic_english`**. \n",
    "  - `weighted`: Whether or not the dataset should be weighted to oversample minority classes.\n",
    "    - **With this dataset, it greatly enhances training so it is turned on.**\n",
    "  \n",
    "\n",
    "\n",
    "- Model\n",
    "- \n",
    "- Training\n",
    "  - `epochs`: How many times through the dataset to train. Depends on time/resources available. \n",
    "    - **Starting with 25 epochs to evaluate training results and will adjust from there.**\n",
    "  - `learning_rate`: The learning rate directly impacts the optimizer, but due to the decoupled nature, can be changed fluidly.\n",
    "  - `learning_rate_scheduler`: The methodology for updating the learning rate. Also one of the ever changing parameters we try to optimize on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Custom Modules - Source Code**\n",
    "A number of custom code was generated to support this experiment. \n",
    "\n",
    "### datasets.py\n",
    "```python\n",
    "import io\n",
    "from typing import Callable, List\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.utils import unicode_csv_reader\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "_default_tokenizer = get_tokenizer(\"basic_english\")\n",
    "DEFAULT_LABEL_TRANSFORM = lambda x: x\n",
    "DEFAULT_TEXT_TRANSFORM = lambda x: _default_tokenizer(x)\n",
    "\n",
    "\n",
    "def create_torch_dataloader(\n",
    "    dataset: data.Dataset,\n",
    "    vocab: Vocab,\n",
    "    label_transform: Callable = DEFAULT_LABEL_TRANSFORM,\n",
    "    text_transform: Callable = DEFAULT_TEXT_TRANSFORM,\n",
    "    weighted=True,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Creates a Pytorch style dataloader using a dataset and a precompiled vocab.\n",
    "\n",
    "    The dataset returns \"model-ready\" data.\n",
    "\n",
    "    Args:\n",
    "        dataset: The raw text dataset to be used during inference\n",
    "        vocab: the premade vocabulary used to index words/phrases\n",
    "        label_transform: any operation used on the datasets label output\n",
    "        text_transform: operation used on the raw text sentence outputs from the data\n",
    "        weighted: whether to weight the samples based on class distribution\n",
    "        **kwargs: any additional kwargs used by Pytorch DataLoaders.\n",
    "\n",
    "    Returns:\n",
    "        A PyTorch DataLoader to be used during training, eval, or test.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _collate_batch(batch):\n",
    "        label_list, docid_list, text_list, offsets = [], [], [], [0]\n",
    "        for (_label, _docid, _text) in batch:\n",
    "            label_list.append(label_transform(_label))\n",
    "            processed_text = torch.tensor(\n",
    "                vocab(text_transform(_text)), dtype=torch.int64\n",
    "            )\n",
    "            text_list.append(processed_text)\n",
    "            offsets.append(processed_text.size(0))\n",
    "            docid_list.append(_docid)\n",
    "        label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "        text_list = torch.cat(text_list)\n",
    "        return label_list.to(device), text_list.to(device), offsets.to(device), docid_list\n",
    "\n",
    "    if weighted:\n",
    "        weights = dataset.sample_weights\n",
    "        sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights))\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    return data.DataLoader(\n",
    "        dataset,\n",
    "        collate_fn=_collate_batch,\n",
    "        shuffle=(sampler is None),\n",
    "        sampler=sampler,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "class TSVRawTextIterableDataset(data.IterableDataset):\n",
    "    \"\"\"Dataset that loads TSV data incrementally as an iterable and returns raw text.\n",
    "\n",
    "    This dataset must be traversed in order as it only reads from the TSV file as it is called.\n",
    "    Useful if the size of data is too large to load into memory at once.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str, data_columns: List[int]):\n",
    "        \"\"\"Loads an iterator from a file.\n",
    "\n",
    "        Args:\n",
    "            filepath: location of the .tsv file\n",
    "            data_columns: the columns in the .tsv that are used as feature data\n",
    "        \"\"\"\n",
    "        self._number_of_items = _get_tsv_file_length(filepath)\n",
    "        self._iterator = _create_data_from_tsv(\n",
    "            filepath, data_column_indices=data_columns\n",
    "        )\n",
    "        self._current_position = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        item = next(self._iterator)\n",
    "        self._current_position += 1\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._number_of_items\n",
    "\n",
    "\n",
    "class TSVRawTextMapDataset(data.Dataset):\n",
    "    \"\"\"Dataset that loads all TSV data into memory and returns raw text.\n",
    "\n",
    "    This dataset provides a map interface, allowing access to any entry.\n",
    "    Useful for modifying the sampling or order during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str, data_columns: List[int]):\n",
    "        \"\"\"Loads .tsv structed data into memory.\n",
    "\n",
    "        Args:\n",
    "            filepath: location of the .tsv file\n",
    "            data_columns: the columns in the .tsv that are used as feature data\n",
    "        \"\"\"\n",
    "        self._records = list(\n",
    "            _create_data_from_tsv(filepath, data_column_indices=data_columns)\n",
    "        )\n",
    "        self._sample_weights, self._class_weights = self._calculate_weights()\n",
    "\n",
    "    @property\n",
    "    def sample_weights(self):\n",
    "        return self._sample_weights\n",
    "    \n",
    "    @property\n",
    "    def class_weights(self):\n",
    "        return self._class_weights\n",
    "\n",
    "    def _calculate_weights(self):\n",
    "        targets = torch.tensor(\n",
    "            [label if label > 0 else 0 for label, *_ in self._records]\n",
    "        )\n",
    "        unique, sample_counts = torch.unique(targets, return_counts=True)\n",
    "        weight = 1.0 / sample_counts\n",
    "        sample_weights =  torch.tensor([weight[t] for t in targets])\n",
    "        class_weights = weight / weight.sum()\n",
    "        return sample_weights, class_weights\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._records[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._records)\n",
    "\n",
    "\n",
    "def _create_data_from_tsv(data_path, data_column_indices):\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        reader = unicode_csv_reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            data = [row[i] for i in data_column_indices]\n",
    "            yield int(row[0]), row[1], \" \".join(data)\n",
    "\n",
    "\n",
    "def _get_tsv_file_length(data_path):\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        row_count = sum(1 for row in f)\n",
    "\n",
    "    return row_count\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Walkthrough\n",
    "\n",
    "We will now walk through the experiment notebook to see results in action.\n",
    "\n",
    "### Imports\n",
    "\n",
    "The open source and custom modules used are imported first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ir_classification import datasets, models\n",
    "from ir_classification import vocab as ir_vocab\n",
    "from ir_classification import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ir-classification-_Pgcz6ju-py3.9\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "datafield_map = {\"assessment\": 0, \"doc_id\": 1, \"title\": 2, \"authors\": 3, \"journal\": 4, \"issn\": 5, \"year\": 6, \"language\": 7, \"abstract\": 8, \"keywords\": 9}\n",
    "data_columns = [datafield_map[\"title\"]]\n",
    "ngrams = 1\n",
    "batch_size = 64\n",
    "\n",
    "# Create vocab from the training data.\n",
    "# vocab = ir_vocab.create_vocab_from_tsv(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns, ngrams=ngrams)\n",
    "glove = ir_vocab.create_glove_with_unk_vector()\n",
    "vocab = ir_vocab.create_vocab_from_glove(glove)\n",
    "\n",
    "# Load the TSV into datasets with the appropriate feature columns.\n",
    "train_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns)\n",
    "val_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.dev.shuf.tsv\", data_columns)\n",
    "\n",
    "# Create the transforms for the dataloader to appropriately format the contents of the files.\n",
    "label_transform = lambda x: x if x > 0 else 0\n",
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "text_transform = lambda x: list(ngrams_iterator(tokenizer(x), ngrams))\n",
    "\n",
    "# Instantiate the dataloaders.\n",
    "train_dataloader = datasets.create_torch_dataloader(train_dataset, vocab,  label_transform, text_transform, weighted=True, batch_size=batch_size)\n",
    "val_dataloader = datasets.create_torch_dataloader(val_dataset, vocab, label_transform, text_transform, weighted=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "vocab_size = len(vocab) # from vocab created earlier.\n",
    "embedding_size = 64\n",
    "hidden_layer_size = 25\n",
    "\n",
    "# Enable compatability when training with GPU enabled devices.  \n",
    "# (Some development work was done in Google Colab with GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = models.EmbeddingBagLinearModel(vocab_size, embedding_size, num_classes).to(device)\n",
    "#model = models.PretrainedEmbeddingMLPModel(num_classes, hidden_layer_size, glove.vectors)\n",
    "model = models.EmbeddingBagMLPModel(num_class=num_classes, hidden_layer_size=100, embedding_vectors=glove.vectors, dropout=0.5)\n",
    "# Free up memory\n",
    "# del glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the top-level training loop\n",
    "\n",
    "The custom code was meant to handle the individual `step` and `epoch` levels generically.\n",
    "This setup should let us change the components experimentally in cells like below without much other hassle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "           Epoch 0: 100%|██████████| 340/340 [00:05<00:00, 67.54 batch/s, accurracy=0.5, loss=0.742]\n",
      "     Validation: 0: 100%|██████████| 76/76 [00:00<00:00, 78.26 batch/s, accurracy=0.52, loss=0.68]\n",
      "           Epoch 1: 100%|██████████| 340/340 [00:03<00:00, 89.49 batch/s, accurracy=0.75, loss=0.583]\n",
      "     Validation: 1: 100%|██████████| 76/76 [00:00<00:00, 112.56 batch/s, accurracy=0.64, loss=0.62]\n",
      "           Epoch 2: 100%|██████████| 340/340 [00:03<00:00, 95.44 batch/s, accurracy=0.25, loss=0.83]\n",
      "     Validation: 2:  36%|███▌      | 27/76 [00:00<00:00, 133.34 batch/s, accurracy=0.75, loss=0.616] "
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Create the loss function weighted to inverse class distribution\n",
    "# loss_function = torch.nn.CrossEntropyLoss(weight=train_dataset.class_weights)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate a Stochastic Gradient Descent optimizer and \"Auto\" Learning Rate schedule.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# Tensorboard writing utility class.\n",
    "log_dir = \"runs/MLPModel\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Perform Training\n",
    "for i in range(EPOCHS):\n",
    "    start_iter = len(train_dataloader) * i\n",
    "    train.train_epoch(i, model, optimizer, loss_function, train_dataloader, start_iter=start_iter, writer=writer)\n",
    "    validation_results = train.evaluate_epoch(i, model, loss_function, val_dataloader, writer)\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "#torch.save(model.state_dict(), \"model_weights/title_only_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "\n",
    "# %tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Metrics on the Dev Set\n",
    "Here we recreate the `dev` dataloader to go a single document at a time.\n",
    "\n",
    "We use the less robust `predict` method so we can explicitly show the values and calculations being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataloader = datasets.create_torch_dataloader(val_dataset, vocab,  label_transform, text_transform, weighted=False, batch_size=1)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "labels = []\n",
    "for batch in dev_dataloader:\n",
    "    label, text, *_ = batch\n",
    "    pred_label = train.predict(model, text)\n",
    "    preds.append(pred_label)\n",
    "    labels.append(label.cpu().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Confusion Matrix\n",
    "\n",
    "We can use the confusion matrix to make it easy to visualize the values for the metrics.  \n",
    "       \n",
    "\n",
    "       \n",
    "|   | P0  | P1  |\n",
    "|---|----|----|\n",
    "| A0 | TN | FP |\n",
    "| A1 | FN | TP |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3773  927]\n",
      " [  33  117]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, average_precision_score\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Precision, Recall, F1-Score on Dev Set\n",
    "We use a confusion matrix to make it easy to map out the values for true positive, true negative, false positive, and false negative.\n",
    "\n",
    "- Precision = tp / (tp + fp)\n",
    "- Recall = tp / (tp + fn)\n",
    "- F1 Score =  2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 117 / (117 + 927) = 0.1121\n",
      "Recall: 117 / (117 + 33) = 0.78\n",
      "F1: 2 * (0.1121 * 0.78) / (0.1121 + 0.78) = 0.196\n",
      "AP: 0.09421791681478849\n",
      "AP-weighted: 0.09421791681478849\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = cm.ravel() # Extract the components\n",
    "\n",
    "# Calculate and print Precision\n",
    "precision_string = f\"{tp} / ({tp} + {fp})\"\n",
    "precision = round(eval(precision_string), 4)\n",
    "print(f\"Precision: {precision_string} = {precision}\")\n",
    "\n",
    "# Calculate and print Recall\n",
    "recall_string = f\"{tp} / ({tp} + {fn})\"\n",
    "recall = round(eval(recall_string), 4)\n",
    "print(f\"Recall: {recall_string} = {recall}\")\n",
    "\n",
    "# Calculate and print F1 Score\n",
    "f1_string = f\"2 * ({precision} * {recall}) / ({precision} + {recall})\"\n",
    "f1 = eval(f1_string)\n",
    "print(f\"F1: {f1_string} = {round(f1, 4)}\")\n",
    "\n",
    "print(f\"AP: {average_precision_score(labels, preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat Experiment with \"Title\", \"Abstract\", and \"Keyword\" Data\n",
    "\n",
    "We will keep everything exactly the same for setup, changing only the things needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_columns = [\"title\", \"abstract\", \"keywords\"]\n",
    "data_columns = [datafield_map[col] for col in use_columns]\n",
    "ngrams = 1\n",
    "\n",
    "#vocab = ir_vocab.create_vocab_from_tsv(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns, ngrams=ngrams)\n",
    "\n",
    "# Load the TSV into datasets with the appropriate feature columns.\n",
    "train_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns)\n",
    "val_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.dev.shuf.tsv\", data_columns)\n",
    "\n",
    "# Instantiate the dataloaders.\n",
    "train_dataloader = datasets.create_torch_dataloader(train_dataset, vocab,  label_transform, text_transform, weighted=True, batch_size=batch_size)\n",
    "val_dataloader = datasets.create_torch_dataloader(val_dataset, vocab,  label_transform, text_transform, weighted=False, batch_size=batch_size)\n",
    "\n",
    "# Create Model\n",
    "vocab_size = len(vocab)\n",
    "# model = models.EmbeddingBagLinearModel(vocab_size, embedding_size, num_classes).to(device)\n",
    "#model = models.PretrainedEmbeddingMLPModel(num_classes, hidden_layer_size, glove.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model_weights/tak_state_dict.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "           Epoch 0: 100%|██████████| 340/340 [00:26<00:00, 12.88 batch/s, accurracy=0, loss=1.1]\n",
      "     Validation: 0: 100%|██████████| 76/76 [00:07<00:00, 10.47 batch/s, accurracy=0.58, loss=0.713]\n",
      "           Epoch 1: 100%|██████████| 340/340 [00:21<00:00, 15.61 batch/s, accurracy=0.75, loss=0.451]\n",
      "     Validation: 1: 100%|██████████| 76/76 [00:03<00:00, 22.89 batch/s, accurracy=0.62, loss=0.691]\n",
      "           Epoch 2: 100%|██████████| 340/340 [00:18<00:00, 18.51 batch/s, accurracy=0.25, loss=1.03]\n",
      "     Validation: 2: 100%|██████████| 76/76 [00:03<00:00, 23.72 batch/s, accurracy=0.56, loss=0.761]\n",
      "           Epoch 3: 100%|██████████| 340/340 [00:18<00:00, 18.51 batch/s, accurracy=1, loss=0.349]\n",
      "     Validation: 3: 100%|██████████| 76/76 [00:03<00:00, 20.51 batch/s, accurracy=0.52, loss=0.724]\n",
      "           Epoch 4: 100%|██████████| 340/340 [00:17<00:00, 19.55 batch/s, accurracy=0.75, loss=0.641]\n",
      "     Validation: 4: 100%|██████████| 76/76 [00:03<00:00, 24.55 batch/s, accurracy=0.68, loss=0.573]\n",
      "           Epoch 5: 100%|██████████| 340/340 [00:15<00:00, 21.77 batch/s, accurracy=0.5, loss=0.676]\n",
      "     Validation: 5: 100%|██████████| 76/76 [00:03<00:00, 22.11 batch/s, accurracy=0.54, loss=0.658]\n",
      "           Epoch 6: 100%|██████████| 340/340 [00:16<00:00, 20.97 batch/s, accurracy=0.5, loss=0.633]\n",
      "     Validation: 6: 100%|██████████| 76/76 [00:03<00:00, 23.25 batch/s, accurracy=0.58, loss=0.639]\n",
      "           Epoch 7: 100%|██████████| 340/340 [00:16<00:00, 20.98 batch/s, accurracy=0.75, loss=0.566]\n",
      "     Validation: 7: 100%|██████████| 76/76 [00:03<00:00, 19.20 batch/s, accurracy=0.56, loss=0.742]\n",
      "           Epoch 8: 100%|██████████| 340/340 [00:15<00:00, 21.95 batch/s, accurracy=0.75, loss=0.349]\n",
      "     Validation: 8: 100%|██████████| 76/76 [00:03<00:00, 22.29 batch/s, accurracy=0.6, loss=0.694]\n",
      "           Epoch 9: 100%|██████████| 340/340 [00:14<00:00, 22.75 batch/s, accurracy=0.25, loss=0.892]\n",
      "     Validation: 9: 100%|██████████| 76/76 [00:02<00:00, 26.62 batch/s, accurracy=0.56, loss=0.77]\n",
      "          Epoch 10: 100%|██████████| 340/340 [00:14<00:00, 23.15 batch/s, accurracy=0.5, loss=0.665]\n",
      "    Validation: 10: 100%|██████████| 76/76 [00:03<00:00, 22.76 batch/s, accurracy=0.68, loss=0.624]\n",
      "          Epoch 11: 100%|██████████| 340/340 [00:13<00:00, 24.71 batch/s, accurracy=0.5, loss=0.721]\n",
      "    Validation: 11: 100%|██████████| 76/76 [00:02<00:00, 27.97 batch/s, accurracy=0.62, loss=0.63]\n",
      "          Epoch 12: 100%|██████████| 340/340 [00:12<00:00, 28.21 batch/s, accurracy=0.75, loss=0.464]\n",
      "    Validation: 12: 100%|██████████| 76/76 [00:02<00:00, 30.50 batch/s, accurracy=0.62, loss=0.68]\n",
      "          Epoch 13: 100%|██████████| 340/340 [00:14<00:00, 23.76 batch/s, accurracy=0.5, loss=0.642]\n",
      "    Validation: 13: 100%|██████████| 76/76 [00:02<00:00, 28.24 batch/s, accurracy=0.76, loss=0.537]\n",
      "          Epoch 14: 100%|██████████| 340/340 [00:11<00:00, 28.40 batch/s, accurracy=0.5, loss=0.854]\n",
      "    Validation: 14: 100%|██████████| 76/76 [00:02<00:00, 27.15 batch/s, accurracy=0.68, loss=0.515]\n",
      "          Epoch 15: 100%|██████████| 340/340 [00:13<00:00, 25.52 batch/s, accurracy=0.5, loss=0.951]\n",
      "    Validation: 15: 100%|██████████| 76/76 [00:02<00:00, 28.65 batch/s, accurracy=0.74, loss=0.52]\n",
      "          Epoch 16: 100%|██████████| 340/340 [00:12<00:00, 26.86 batch/s, accurracy=0.75, loss=0.769]\n",
      "    Validation: 16: 100%|██████████| 76/76 [00:02<00:00, 27.95 batch/s, accurracy=0.64, loss=0.59]\n",
      "          Epoch 17: 100%|██████████| 340/340 [00:14<00:00, 23.51 batch/s, accurracy=1, loss=0.293]\n",
      "    Validation: 17: 100%|██████████| 76/76 [00:03<00:00, 24.02 batch/s, accurracy=0.68, loss=0.618]\n",
      "          Epoch 18: 100%|██████████| 340/340 [00:15<00:00, 22.51 batch/s, accurracy=0.75, loss=0.641]\n",
      "    Validation: 18: 100%|██████████| 76/76 [00:03<00:00, 24.14 batch/s, accurracy=0.7, loss=0.591]\n",
      "          Epoch 19: 100%|██████████| 340/340 [00:13<00:00, 24.36 batch/s, accurracy=0.75, loss=0.503]\n",
      "    Validation: 19: 100%|██████████| 76/76 [00:02<00:00, 29.92 batch/s, accurracy=0.68, loss=0.53]\n",
      "          Epoch 20: 100%|██████████| 340/340 [00:11<00:00, 29.55 batch/s, accurracy=0.75, loss=0.315]\n",
      "    Validation: 20: 100%|██████████| 76/76 [00:02<00:00, 30.56 batch/s, accurracy=0.74, loss=0.5]\n",
      "          Epoch 21: 100%|██████████| 340/340 [00:12<00:00, 28.30 batch/s, accurracy=0.75, loss=0.531]\n",
      "    Validation: 21: 100%|██████████| 76/76 [00:02<00:00, 27.69 batch/s, accurracy=0.78, loss=0.497]\n",
      "          Epoch 22: 100%|██████████| 340/340 [00:11<00:00, 29.01 batch/s, accurracy=0.75, loss=0.495]\n",
      "    Validation: 22: 100%|██████████| 76/76 [00:02<00:00, 30.72 batch/s, accurracy=0.7, loss=0.562]\n",
      "          Epoch 23: 100%|██████████| 340/340 [00:12<00:00, 26.79 batch/s, accurracy=1, loss=0.342]\n",
      "    Validation: 23: 100%|██████████| 76/76 [00:03<00:00, 25.19 batch/s, accurracy=0.68, loss=0.608]\n",
      "          Epoch 24: 100%|██████████| 340/340 [00:13<00:00, 25.13 batch/s, accurracy=1, loss=0.373]\n",
      "    Validation: 24: 100%|██████████| 76/76 [00:02<00:00, 27.76 batch/s, accurracy=0.7, loss=0.507]\n",
      "          Epoch 25: 100%|██████████| 340/340 [00:13<00:00, 25.01 batch/s, accurracy=0.75, loss=0.592]\n",
      "    Validation: 25: 100%|██████████| 76/76 [00:02<00:00, 27.50 batch/s, accurracy=0.62, loss=0.639]\n",
      "          Epoch 26: 100%|██████████| 340/340 [00:13<00:00, 24.36 batch/s, accurracy=1, loss=0.294]\n",
      "    Validation: 26: 100%|██████████| 76/76 [00:02<00:00, 25.95 batch/s, accurracy=0.7, loss=0.574]\n",
      "          Epoch 27: 100%|██████████| 340/340 [00:13<00:00, 25.14 batch/s, accurracy=1, loss=0.308]\n",
      "    Validation: 27: 100%|██████████| 76/76 [00:02<00:00, 26.25 batch/s, accurracy=0.64, loss=0.588]\n",
      "          Epoch 28: 100%|██████████| 340/340 [00:13<00:00, 25.39 batch/s, accurracy=0.5, loss=0.689]\n",
      "    Validation: 28: 100%|██████████| 76/76 [00:02<00:00, 26.36 batch/s, accurracy=0.68, loss=0.532]\n",
      "          Epoch 29: 100%|██████████| 340/340 [00:13<00:00, 25.58 batch/s, accurracy=0.5, loss=0.55]\n",
      "    Validation: 29: 100%|██████████| 76/76 [00:02<00:00, 26.49 batch/s, accurracy=0.74, loss=0.561]\n",
      "          Epoch 30: 100%|██████████| 340/340 [00:13<00:00, 25.32 batch/s, accurracy=0.75, loss=0.477]\n",
      "    Validation: 30: 100%|██████████| 76/76 [00:02<00:00, 26.34 batch/s, accurracy=0.64, loss=0.674]\n",
      "          Epoch 31: 100%|██████████| 340/340 [00:13<00:00, 24.92 batch/s, accurracy=1, loss=0.358]\n",
      "    Validation: 31: 100%|██████████| 76/76 [00:02<00:00, 26.78 batch/s, accurracy=0.74, loss=0.556]\n",
      "          Epoch 32: 100%|██████████| 340/340 [00:13<00:00, 25.31 batch/s, accurracy=0.75, loss=0.513]\n",
      "    Validation: 32: 100%|██████████| 76/76 [00:02<00:00, 26.50 batch/s, accurracy=0.72, loss=0.533]\n",
      "          Epoch 33: 100%|██████████| 340/340 [00:13<00:00, 25.28 batch/s, accurracy=1, loss=0.396]\n",
      "    Validation: 33: 100%|██████████| 76/76 [00:03<00:00, 24.04 batch/s, accurracy=0.7, loss=0.542]\n",
      "          Epoch 34: 100%|██████████| 340/340 [00:13<00:00, 25.10 batch/s, accurracy=0.75, loss=0.465]\n",
      "    Validation: 34: 100%|██████████| 76/76 [00:02<00:00, 26.66 batch/s, accurracy=0.78, loss=0.441]\n",
      "          Epoch 35: 100%|██████████| 340/340 [00:13<00:00, 24.52 batch/s, accurracy=1, loss=0.308]\n",
      "    Validation: 35: 100%|██████████| 76/76 [00:02<00:00, 26.81 batch/s, accurracy=0.8, loss=0.439]\n",
      "          Epoch 36: 100%|██████████| 340/340 [00:13<00:00, 25.04 batch/s, accurracy=0.75, loss=0.431]\n",
      "    Validation: 36: 100%|██████████| 76/76 [00:02<00:00, 26.99 batch/s, accurracy=0.76, loss=0.504]\n",
      "          Epoch 37: 100%|██████████| 340/340 [00:13<00:00, 25.18 batch/s, accurracy=1, loss=0.302]\n",
      "    Validation: 37: 100%|██████████| 76/76 [00:02<00:00, 27.29 batch/s, accurracy=0.68, loss=0.498]\n",
      "          Epoch 38: 100%|██████████| 340/340 [00:13<00:00, 25.20 batch/s, accurracy=1, loss=0.348]\n",
      "    Validation: 38: 100%|██████████| 76/76 [00:02<00:00, 26.32 batch/s, accurracy=0.7, loss=0.563]\n",
      "          Epoch 39: 100%|██████████| 340/340 [00:14<00:00, 23.82 batch/s, accurracy=1, loss=0.396]\n",
      "    Validation: 39: 100%|██████████| 76/76 [00:03<00:00, 23.28 batch/s, accurracy=0.78, loss=0.55]\n",
      "          Epoch 40: 100%|██████████| 340/340 [00:13<00:00, 25.62 batch/s, accurracy=1, loss=0.217]\n",
      "    Validation: 40: 100%|██████████| 76/76 [00:02<00:00, 25.84 batch/s, accurracy=0.78, loss=0.515]\n",
      "          Epoch 41: 100%|██████████| 340/340 [00:13<00:00, 24.91 batch/s, accurracy=0.75, loss=0.432]\n",
      "    Validation: 41: 100%|██████████| 76/76 [00:03<00:00, 25.10 batch/s, accurracy=0.72, loss=0.552]\n",
      "          Epoch 42: 100%|██████████| 340/340 [00:13<00:00, 24.60 batch/s, accurracy=1, loss=0.283]\n",
      "    Validation: 42: 100%|██████████| 76/76 [00:02<00:00, 27.39 batch/s, accurracy=0.74, loss=0.524]\n",
      "          Epoch 43: 100%|██████████| 340/340 [00:13<00:00, 24.97 batch/s, accurracy=0.5, loss=0.735]\n",
      "    Validation: 43: 100%|██████████| 76/76 [00:02<00:00, 27.66 batch/s, accurracy=0.66, loss=0.597]\n",
      "          Epoch 44: 100%|██████████| 340/340 [00:13<00:00, 25.42 batch/s, accurracy=0.5, loss=0.6]\n",
      "    Validation: 44: 100%|██████████| 76/76 [00:02<00:00, 26.62 batch/s, accurracy=0.78, loss=0.417]\n",
      "          Epoch 45: 100%|██████████| 340/340 [00:13<00:00, 25.61 batch/s, accurracy=0.75, loss=0.861]\n",
      "    Validation: 45: 100%|██████████| 76/76 [00:02<00:00, 26.52 batch/s, accurracy=0.62, loss=0.594]\n",
      "          Epoch 46: 100%|██████████| 340/340 [00:13<00:00, 25.66 batch/s, accurracy=0.25, loss=0.66]\n",
      "    Validation: 46: 100%|██████████| 76/76 [00:02<00:00, 26.77 batch/s, accurracy=0.66, loss=0.59]\n",
      "          Epoch 47: 100%|██████████| 340/340 [00:13<00:00, 25.16 batch/s, accurracy=0.75, loss=0.545]\n",
      "    Validation: 47: 100%|██████████| 76/76 [00:02<00:00, 27.51 batch/s, accurracy=0.82, loss=0.46]\n",
      "          Epoch 48: 100%|██████████| 340/340 [00:13<00:00, 24.90 batch/s, accurracy=0.75, loss=0.564]\n",
      "    Validation: 48: 100%|██████████| 76/76 [00:02<00:00, 26.66 batch/s, accurracy=0.72, loss=0.48]\n",
      "          Epoch 49: 100%|██████████| 340/340 [00:13<00:00, 25.01 batch/s, accurracy=0.75, loss=0.53]\n",
      "    Validation: 49: 100%|██████████| 76/76 [00:02<00:00, 26.53 batch/s, accurracy=0.8, loss=0.486]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Create the loss function weighted to inverse class distribution\n",
    "# loss_function = torch.nn.CrossEntropyLoss(weight=train_dataset.class_weights)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate a Stochastic Gradient Descent optimizer and \"Auto\" Learning Rate schedule.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"max\")\n",
    "\n",
    "# Tensorboard writing utility class.\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Perform Training\n",
    "for i in range(EPOCHS):\n",
    "    start_iter = len(train_dataloader) * i\n",
    "    train.train_epoch(i, model, optimizer, loss_function, train_dataloader, start_iter=start_iter, writer=writer)\n",
    "    validation_results = train.evaluate_epoch(i, model, loss_function, val_dataloader, writer)\n",
    "    # scheduler.step(validation_results[\"recall\"])\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3460 1240]\n",
      " [  34  116]]\n",
      "Precision: 116 / (116 + 1240) = 0.0855\n",
      "Recall: 116 / (116 + 34) = 0.7733\n",
      "F1: 2 * (0.0855 * 0.7733) / (0.0855 + 0.7733) = 0.154\n",
      "AP: 0.07316566817707225\n"
     ]
    }
   ],
   "source": [
    "dev_dataloader = datasets.create_torch_dataloader(val_dataset, vocab,  label_transform, text_transform, weighted=False, batch_size=1)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "labels = []\n",
    "for batch in dev_dataloader:\n",
    "    label, text, *_ = batch\n",
    "    pred_label = train.predict(model, text)\n",
    "    preds.append(pred_label)\n",
    "    labels.append(label.cpu().item())\n",
    "\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(cm)\n",
    "tn, fp, fn, tp = cm.ravel() # Extract the components\n",
    "\n",
    "# Calculate and print Precision\n",
    "precision_string = f\"{tp} / ({tp} + {fp})\"\n",
    "precision = round(eval(precision_string), 4)\n",
    "print(f\"Precision: {precision_string} = {precision}\")\n",
    "\n",
    "# Calculate and print Recall\n",
    "recall_string = f\"{tp} / ({tp} + {fn})\"\n",
    "recall = round(eval(recall_string), 4)\n",
    "print(f\"Recall: {recall_string} = {recall}\")\n",
    "\n",
    "# Calculate and print F1 Score\n",
    "f1_string = f\"2 * ({precision} * {recall}) / ({precision} + {recall})\"\n",
    "f1 = eval(f1_string)\n",
    "print(f\"F1: {f1_string} = {round(f1, 4)}\")\n",
    "\n",
    "\n",
    "print(f\"AP: {average_precision_score(labels, preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_weights/tak_state_dict.pth\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2c370576f24a547f2bd7f1badbf1e39ac1d010b680ddb4523878c870f4696a1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ir-classification-_Pgcz6ju-py3.9': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
