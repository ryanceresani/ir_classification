{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #4\n",
    "\n",
    "**Authors:** *Ryan Ceresani*\n",
    "\n",
    "**Class:** *605.744 Information Retrieval*\n",
    "\n",
    "**Term:** *Fall 2021*\n",
    "\n",
    "This assignment submission is structured slightly different than my previous - as we are using a Notebook in place of a \"main\" application. \n",
    "The notebook provides a nice integration for intermediate code output, plotting, results and experimentation.\n",
    "\n",
    "## Expected Deliverables\n",
    "- Report outlining methodologies, tools used, parameter decisions, etc.\n",
    "- Precision, Recall, F1 score for the `\"dev\"` dataset using \"Title\" column as features.\n",
    "- Metrics for `\"dev\"` dataset using \"Title\", \"Abstract\", and \"Keywords\" as features.\n",
    "- Perform an additional non-trival experimentation.\n",
    "- Predictions for the `\"test\"` dataset.\n",
    "\n",
    "\n",
    "## **Overall Methodology**\n",
    "Before getting into code specifics, we will address some overall elements used during this assignment. The main approach I am choosing to perform **Text Classification** is through deep learning.\n",
    "\n",
    "### Open Source Libraries\n",
    "To facility deep learning, this experiment makes heavy use of `PyTorch` and `torchtext` for doing the underlying operations.  Additionally, `scikit-learn` is useful for their `metrics` library which offers useful calculation ability for a variety of classifation metrics (whether you use one of their estimators or not.)  \n",
    "- **PyTorch**: This is the heavy-lifter for a number of backing abstract classes performing a variety of functions.\n",
    "    - `data`: We created custom PyTorch `Dataset` and `DataLoader` classes for batching, sampling, and shuffling our training, validation, and test data as appropriate.\n",
    "    - `nn.Module`: This is the abstract base class for most things in `PyTorch` but is the foundation for any model or classifier we will use.\n",
    "    - `optim`: This provides an optimizer and learning rate scheduler for the training loops. One of the powerful utilities of PyTorch is the way it hides gradient interactions and backward propagation from a user.\n",
    "    - `loss`: Combined with `optim` the loss module provides the ability to generate loss from our predictions. Specifically we use weighted CrossEntropyLoss.\n",
    "    - **General PyTorch magic:** PyTorch also provides some other nice things when it comes to the gradient operations being attached to tensors or the built-in CUDA support. \n",
    "-  **Torchtext**: Extension off the official PyTorch to provide text based utilities.\n",
    "   -  `vocab`: This generates a `Vocabulary` object from an iterator which can be used to map words to indices, converting tokens into values.\n",
    "   -  `utils`: It also has some built in utilities for tokenizing and creating ngrams.\n",
    "- **scikit-learn**: Used for the `metrics` library to generate scores and repots.\n",
    "\n",
    "### **Dataset**\n",
    "Systematic Review\n",
    "\n",
    "- NOTE: The Document **hash:a8113f0b-6561-3178-8c2d-7b4ebac229ff** contained an odd sequence of characters in UTF8 at the beginning of the \"Article\" section which caused problems for the python stdlib `csv_reader`. The value was sanitized to remove the characters (`\",`) - which would be removed in tokenization anyway. \n",
    "\n",
    "#### Dataset Challenges\n",
    "The primary challenge imposed by this dataset is the vast class imbalance within training. (30:1 negative skew)\n",
    "\n",
    "To counteract this, two approaches were used in tandem:\n",
    "1. The chosen loss function (`CrossEntropyLoss`) used inverse class frequency weighting to encourage learning on the minority class.\n",
    "2. The `DataLoader` used a `WeightedRandomSampler` which was weighted to essentially up-sample the minority class and create an artificial balance.\n",
    "\n",
    "### **Major Parameters**\n",
    "Across all of the libraries and custom code (to be shown below) there are a number of key parameters that influence the results. They will be broken down into categories.\n",
    "\n",
    "- Data\n",
    "  - `batch_size`: Batch size when processing data can impact the quality of training depending on application.\n",
    "    - **A batch size of 32 was used to balance between training speed and overgeneralization.**\n",
    "  - `data_columns`: This specifies which columns of the *.tsv* file are to be used as features. \n",
    "    - **The prompt specifies we use first *text* and then *text + abstract + keyword***\n",
    "  - `ngrams`: how large the ngram value should be for the dataset. \n",
    "    - **Initial value set to `unigram` or 1.**\n",
    "  - `tokenizer`: `torchtext` comes with a number of pre-made tokenizers, each having slight variations. \n",
    "    - **For the initial pass we will use `basic_english`**. \n",
    "  - `weighted`: Whether or not the dataset should be weighted to oversample minority classes.\n",
    "    - **With this dataset, it greatly enhances training so it is turned on.**\n",
    "  \n",
    "\n",
    "\n",
    "- Model\n",
    "  - The model itself counts as a parameter and in this case it is as a very simple architecture with an `EmbeddingBag` and a fully connected linear layer.\n",
    "    - The hyperparameters for the model are:\n",
    "      - `vocab_size`: Which is directly tied to the dataset and not exactly a parameter.\n",
    "      - `embedding_dim`: The size of the embedding and consequently the input to the linear layer.\n",
    "        - **Initially set to 64**\n",
    "  \n",
    "- Training\n",
    "  - `epochs`: How many times through the dataset to train. Depends on time/resources available. \n",
    "    - **Starting with 25 epochs to evaluate training results and will adjust from there.**\n",
    "  - `learning_rate`: The learning rate directly impacts the optimizer, but due to the decoupled nature, can be changed fluidly.\n",
    "    - **It is starting at a very high value of 5 to encourage faster convergence.**\n",
    "  - `learning_rate_scheduler`: The methodology for updating the learning rate. We will be reducing the learning rate during a plateau of metric values.\n",
    "    - **Have chosen to reduce LR when plateuing on the F1 score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Custom Modules - Source Code**\n",
    "A number of custom code was generated to support this experiment. \n",
    "\n",
    "### datasets.py\n",
    "```python\n",
    "import io\n",
    "from typing import Callable, List\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.utils import unicode_csv_reader\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "_default_tokenizer = get_tokenizer(\"basic_english\")\n",
    "DEFAULT_LABEL_TRANSFORM = lambda x: x\n",
    "DEFAULT_TEXT_TRANSFORM = lambda x: _default_tokenizer(x)\n",
    "\n",
    "\n",
    "def create_torch_dataloader(\n",
    "    dataset: data.Dataset,\n",
    "    vocab: Vocab,\n",
    "    label_transform: Callable = DEFAULT_LABEL_TRANSFORM,\n",
    "    text_transform: Callable = DEFAULT_TEXT_TRANSFORM,\n",
    "    weighted=True,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Creates a Pytorch style dataloader using a dataset and a precompiled vocab.\n",
    "\n",
    "    The dataset returns \"model-ready\" data.\n",
    "\n",
    "    Args:\n",
    "        dataset: The raw text dataset to be used during inference\n",
    "        vocab: the premade vocabulary used to index words/phrases\n",
    "        label_transform: any operation used on the datasets label output\n",
    "        text_transform: operation used on the raw text sentence outputs from the data\n",
    "        weighted: whether to weight the samples based on class distribution\n",
    "        **kwargs: any additional kwargs used by Pytorch DataLoaders.\n",
    "\n",
    "    Returns:\n",
    "        A PyTorch DataLoader to be used during training, eval, or test.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _collate_batch(batch):\n",
    "        label_list, docid_list, text_list, offsets = [], [], [], [0]\n",
    "        for (_label, _docid, _text) in batch:\n",
    "            label_list.append(label_transform(_label))\n",
    "            processed_text = torch.tensor(\n",
    "                vocab(text_transform(_text)), dtype=torch.int64\n",
    "            )\n",
    "            text_list.append(processed_text)\n",
    "            offsets.append(processed_text.size(0))\n",
    "            docid_list.append(_docid)\n",
    "        label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "        text_list = torch.cat(text_list)\n",
    "        return label_list.to(device), text_list.to(device), offsets.to(device), docid_list\n",
    "\n",
    "    if weighted:\n",
    "        weights = dataset.sample_weights\n",
    "        sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights))\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    return data.DataLoader(\n",
    "        dataset,\n",
    "        collate_fn=_collate_batch,\n",
    "        shuffle=(sampler is None),\n",
    "        sampler=sampler,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "class TSVRawTextIterableDataset(data.IterableDataset):\n",
    "    \"\"\"Dataset that loads TSV data incrementally as an iterable and returns raw text.\n",
    "\n",
    "    This dataset must be traversed in order as it only reads from the TSV file as it is called.\n",
    "    Useful if the size of data is too large to load into memory at once.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str, data_columns: List[int]):\n",
    "        \"\"\"Loads an iterator from a file.\n",
    "\n",
    "        Args:\n",
    "            filepath: location of the .tsv file\n",
    "            data_columns: the columns in the .tsv that are used as feature data\n",
    "        \"\"\"\n",
    "        self._number_of_items = _get_tsv_file_length(filepath)\n",
    "        self._iterator = _create_data_from_tsv(\n",
    "            filepath, data_column_indices=data_columns\n",
    "        )\n",
    "        self._current_position = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        item = next(self._iterator)\n",
    "        self._current_position += 1\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._number_of_items\n",
    "\n",
    "\n",
    "class TSVRawTextMapDataset(data.Dataset):\n",
    "    \"\"\"Dataset that loads all TSV data into memory and returns raw text.\n",
    "\n",
    "    This dataset provides a map interface, allowing access to any entry.\n",
    "    Useful for modifying the sampling or order during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str, data_columns: List[int]):\n",
    "        \"\"\"Loads .tsv structed data into memory.\n",
    "\n",
    "        Args:\n",
    "            filepath: location of the .tsv file\n",
    "            data_columns: the columns in the .tsv that are used as feature data\n",
    "        \"\"\"\n",
    "        self._records = list(\n",
    "            _create_data_from_tsv(filepath, data_column_indices=data_columns)\n",
    "        )\n",
    "        self._sample_weights, self._class_weights = self._calculate_weights()\n",
    "\n",
    "    @property\n",
    "    def sample_weights(self):\n",
    "        return self._sample_weights\n",
    "    \n",
    "    @property\n",
    "    def class_weights(self):\n",
    "        return self._class_weights\n",
    "\n",
    "    def _calculate_weights(self):\n",
    "        targets = torch.tensor(\n",
    "            [label if label > 0 else 0 for label, *_ in self._records]\n",
    "        )\n",
    "        unique, sample_counts = torch.unique(targets, return_counts=True)\n",
    "        weight = 1.0 / sample_counts\n",
    "        sample_weights =  torch.tensor([weight[t] for t in targets])\n",
    "        class_weights = weight / weight.sum()\n",
    "        return sample_weights, class_weights\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._records[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._records)\n",
    "\n",
    "\n",
    "def _create_data_from_tsv(data_path, data_column_indices):\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        reader = unicode_csv_reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            data = [row[i] for i in data_column_indices]\n",
    "            yield int(row[0]), row[1], \" \".join(data)\n",
    "\n",
    "\n",
    "def _get_tsv_file_length(data_path):\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        row_count = sum(1 for row in f)\n",
    "\n",
    "    return row_count\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Walkthrough\n",
    "\n",
    "We will now walk through the experiment notebook to see results in action.\n",
    "\n",
    "### Imports\n",
    "\n",
    "The open source and custom modules used are imported first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ir_classification import datasets, models\n",
    "from ir_classification import vocab as ir_vocab\n",
    "from ir_classification import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafield_map = {\"assessment\": 0, \"doc_id\": 1, \"title\": 2, \"authors\": 3, \"journal\": 4, \"issn\": 5, \"year\": 6, \"language\": 7, \"abstract\": 8, \"keywords\": 9}\n",
    "data_columns = [datafield_map[\"title\"]]\n",
    "ngrams = 1\n",
    "batch_size = 64\n",
    "\n",
    "# Create vocab from the training data.\n",
    "# vocab = ir_vocab.create_vocab_from_tsv(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns, ngrams=ngrams)\n",
    "glove = ir_vocab.create_glove_with_unk_vector()\n",
    "vocab = ir_vocab.create_vocab_from_glove(glove)\n",
    "\n",
    "# Load the TSV into datasets with the appropriate feature columns.\n",
    "train_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns)\n",
    "val_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.dev.shuf.tsv\", data_columns)\n",
    "\n",
    "# Create the transforms for the dataloader to appropriately format the contents of the files.\n",
    "label_transform = lambda x: x if x > 0 else 0\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "text_transform = lambda x: list(ngrams_iterator(tokenizer(x), ngrams))\n",
    "\n",
    "# Instantiate the dataloaders.\n",
    "train_dataloader = datasets.create_torch_dataloader(train_dataset, vocab,  label_transform, text_transform, weighted=True, batch_size=batch_size)\n",
    "val_dataloader = datasets.create_torch_dataloader(val_dataset, vocab,  label_transform, text_transform, weighted=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "vocab_size = len(vocab) # from vocab created earlier.\n",
    "embedding_size = 300\n",
    "hidden_layer_size = 10\n",
    "\n",
    "# Enable compatability when training with GPU enabled devices.  \n",
    "# (Some development work was done in Google Colab with GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = models.EmbeddingBagLinearModel(vocab_size, embedding_size, num_classes).to(device)\n",
    "model = models.PretrainedEmbeddingMLPModel(num_classes, hidden_layer_size, glove.vectors)\n",
    "\n",
    "# Free up memory\n",
    "del glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the top-level training loop\n",
    "\n",
    "The custom code was meant to handle the individual `step` and `epoch` levels generically.\n",
    "This setup should let us change the components experimentally in cells like below without much other hassle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "           Epoch 0: 100%|██████████| 340/340 [00:02<00:00, 130.29 batch/s, accurracy=0.75, loss=0.265]\n",
      "     Validation: 0: 100%|██████████| 76/76 [00:00<00:00, 145.98 batch/s, accurracy=0.98, loss=1.23]\n",
      "           Epoch 1: 100%|██████████| 340/340 [00:02<00:00, 127.46 batch/s, accurracy=0.75, loss=0.326]\n",
      "     Validation: 1: 100%|██████████| 76/76 [00:00<00:00, 171.24 batch/s, accurracy=0.92, loss=0.141]\n",
      "           Epoch 2: 100%|██████████| 340/340 [00:02<00:00, 145.09 batch/s, accurracy=0.75, loss=0.43]\n",
      "     Validation: 2: 100%|██████████| 76/76 [00:00<00:00, 175.75 batch/s, accurracy=0.92, loss=1.78]\n",
      "           Epoch 3: 100%|██████████| 340/340 [00:02<00:00, 146.72 batch/s, accurracy=1, loss=0.0864]\n",
      "     Validation: 3: 100%|██████████| 76/76 [00:00<00:00, 170.93 batch/s, accurracy=0.92, loss=0.915]\n",
      "           Epoch 4: 100%|██████████| 340/340 [00:02<00:00, 151.07 batch/s, accurracy=1, loss=0.0967]\n",
      "     Validation: 4: 100%|██████████| 76/76 [00:00<00:00, 153.02 batch/s, accurracy=0.98, loss=0.0545]\n",
      "           Epoch 5: 100%|██████████| 340/340 [00:02<00:00, 141.19 batch/s, accurracy=0.75, loss=0.439]\n",
      "     Validation: 5: 100%|██████████| 76/76 [00:00<00:00, 153.48 batch/s, accurracy=0.94, loss=1.1]\n",
      "           Epoch 6: 100%|██████████| 340/340 [00:02<00:00, 146.96 batch/s, accurracy=0.75, loss=0.236]\n",
      "     Validation: 6: 100%|██████████| 76/76 [00:00<00:00, 172.91 batch/s, accurracy=0.92, loss=0.673]\n",
      "           Epoch 7: 100%|██████████| 340/340 [00:02<00:00, 135.51 batch/s, accurracy=1, loss=0.0396]\n",
      "     Validation: 7: 100%|██████████| 76/76 [00:00<00:00, 193.31 batch/s, accurracy=0.74, loss=0.993]\n",
      "           Epoch 8: 100%|██████████| 340/340 [00:02<00:00, 154.13 batch/s, accurracy=0.5, loss=0.236]\n",
      "     Validation: 8: 100%|██████████| 76/76 [00:00<00:00, 163.18 batch/s, accurracy=0.76, loss=0.357]\n",
      "           Epoch 9: 100%|██████████| 340/340 [00:02<00:00, 146.50 batch/s, accurracy=0.75, loss=0.742]\n",
      "     Validation: 9: 100%|██████████| 76/76 [00:00<00:00, 164.94 batch/s, accurracy=0.92, loss=0.532]\n",
      "          Epoch 10: 100%|██████████| 340/340 [00:02<00:00, 156.70 batch/s, accurracy=0.75, loss=0.921]\n",
      "    Validation: 10: 100%|██████████| 76/76 [00:00<00:00, 195.90 batch/s, accurracy=0.82, loss=0.325]\n",
      "          Epoch 11: 100%|██████████| 340/340 [00:02<00:00, 167.99 batch/s, accurracy=0.5, loss=0.948]\n",
      "    Validation: 11: 100%|██████████| 76/76 [00:00<00:00, 195.90 batch/s, accurracy=0.7, loss=1.34]\n",
      "          Epoch 12: 100%|██████████| 340/340 [00:02<00:00, 162.60 batch/s, accurracy=0.75, loss=0.829]\n",
      "    Validation: 12: 100%|██████████| 76/76 [00:00<00:00, 176.81 batch/s, accurracy=0.94, loss=0.175]\n",
      "          Epoch 13: 100%|██████████| 340/340 [00:02<00:00, 156.02 batch/s, accurracy=1, loss=0.155]\n",
      "    Validation: 13: 100%|██████████| 76/76 [00:00<00:00, 188.16 batch/s, accurracy=0.8, loss=0.569]\n",
      "          Epoch 14: 100%|██████████| 340/340 [00:02<00:00, 151.07 batch/s, accurracy=0.5, loss=0.645]\n",
      "    Validation: 14: 100%|██████████| 76/76 [00:00<00:00, 157.60 batch/s, accurracy=0.9, loss=0.206]\n",
      "          Epoch 15: 100%|██████████| 340/340 [00:02<00:00, 139.85 batch/s, accurracy=0.5, loss=2.44]\n",
      "    Validation: 15: 100%|██████████| 76/76 [00:00<00:00, 148.29 batch/s, accurracy=0.94, loss=0.318]\n",
      "          Epoch 16: 100%|██████████| 340/340 [00:02<00:00, 138.72 batch/s, accurracy=1, loss=0.0884]\n",
      "    Validation: 16: 100%|██████████| 76/76 [00:00<00:00, 158.10 batch/s, accurracy=0.8, loss=0.706]\n",
      "          Epoch 17: 100%|██████████| 340/340 [00:02<00:00, 142.65 batch/s, accurracy=0.75, loss=0.294]\n",
      "    Validation: 17: 100%|██████████| 76/76 [00:00<00:00, 160.08 batch/s, accurracy=0.92, loss=0.248]\n",
      "          Epoch 18: 100%|██████████| 340/340 [00:02<00:00, 141.87 batch/s, accurracy=0.5, loss=0.563]\n",
      "    Validation: 18: 100%|██████████| 76/76 [00:00<00:00, 159.07 batch/s, accurracy=0.96, loss=0.168]\n",
      "          Epoch 19: 100%|██████████| 340/340 [00:02<00:00, 151.00 batch/s, accurracy=0.75, loss=0.321]\n",
      "    Validation: 19: 100%|██████████| 76/76 [00:00<00:00, 165.12 batch/s, accurracy=0.98, loss=0.0283]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "learning_rate = 5\n",
    "\n",
    "# Create the loss function weighted to inverse class distribution\n",
    "# loss_function = torch.nn.CrossEntropyLoss(weight=train_dataset.class_weights)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate a Stochastic Gradient Descent optimizer and \"Auto\" Learning Rate schedule.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# Tensorboard writing utility class.\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Perform Training\n",
    "for i in range(EPOCHS):\n",
    "    start_iter = len(train_dataloader) * i\n",
    "    train.train_epoch(i, model, optimizer, loss_function, train_dataloader, start_iter=start_iter, writer=writer)\n",
    "    validation_results = train.evaluate_epoch(i, model, loss_function, val_dataloader, writer)\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "#torch.save(model.state_dict(), \"model_weights/title_only_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "\n",
    "# %tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Metrics on the Dev Set\n",
    "Here we recreate the `dev` dataloader to go a single document at a time.\n",
    "\n",
    "We use the less robust `predict` method so we can explicitly show the values and calculations being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataloader = datasets.create_torch_dataloader(val_dataset, vocab,  label_transform, text_transform, weighted=False, batch_size=1)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "labels = []\n",
    "for batch in dev_dataloader:\n",
    "    label, text, *_ = batch\n",
    "    pred_label = train.predict(model, text)\n",
    "    preds.append(pred_label)\n",
    "    labels.append(label.cpu().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Confusion Matrix\n",
    "\n",
    "We can use the confusion matrix to make it easy to visualize the values for the metrics.  \n",
    "       \n",
    "\n",
    "       \n",
    "|   | P0  | P1  |\n",
    "|---|----|----|\n",
    "| A0 | TN | FP |\n",
    "| A1 | FN | TP |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4388  312]\n",
      " [  83   67]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, average_precision_score\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Precision, Recall, F1-Score on Dev Set\n",
    "We use a confusion matrix to make it easy to map out the values for true positive, true negative, false positive, and false negative.\n",
    "\n",
    "- Precision = tp / (tp + fp)\n",
    "- Recall = tp / (tp + fn)\n",
    "- F1 Score =  2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 67 / (67 + 312) = 0.1768\n",
      "Recall: 67 / (67 + 83) = 0.4467\n",
      "F1: 2 * (0.1768 * 0.4467) / (0.1768 + 0.4467) = 0.2533\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = cm.ravel() # Extract the components\n",
    "\n",
    "# Calculate and print Precision\n",
    "precision_string = f\"{tp} / ({tp} + {fp})\"\n",
    "precision = round(eval(precision_string), 4)\n",
    "print(f\"Precision: {precision_string} = {precision}\")\n",
    "\n",
    "# Calculate and print Recall\n",
    "recall_string = f\"{tp} / ({tp} + {fn})\"\n",
    "recall = round(eval(recall_string), 4)\n",
    "print(f\"Recall: {recall_string} = {recall}\")\n",
    "\n",
    "# Calculate and print F1 Score\n",
    "f1_string = f\"2 * ({precision} * {recall}) / ({precision} + {recall})\"\n",
    "f1 = eval(f1_string)\n",
    "print(f\"F1: {f1_string} = {round(f1, 4)}\")\n",
    "\n",
    "print(f\"AP: {average_precision_score(labels, preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09607558324039568"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat Experiment with \"Title\", \"Abstract\", and \"Keyword\" Data\n",
    "\n",
    "We will keep everything exactly the same for setup, changing only the things needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_columns = [\"title\", \"abstract\", \"keywords\"]\n",
    "data_columns = [datafield_map[col] for col in use_columns]\n",
    "ngrams = 1\n",
    "\n",
    "# Create vocab from the training data.\n",
    "vocab = ir_vocab.create_vocab_from_tsv(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns, ngrams=ngrams)\n",
    "\n",
    "# Load the TSV into datasets with the appropriate feature columns.\n",
    "train_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.train.shuf.tsv\", data_columns)\n",
    "val_dataset = datasets.TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.dev.shuf.tsv\", data_columns)\n",
    "\n",
    "# Instantiate the dataloaders.\n",
    "train_dataloader = datasets.create_torch_dataloader(train_dataset, vocab,  label_transform, text_transform, weighted=True, batch_size=batch_size)\n",
    "val_dataloader = datasets.create_torch_dataloader(val_dataset, vocab,  label_transform, text_transform, weighted=False, batch_size=batch_size)\n",
    "\n",
    "# Create Model\n",
    "vocab_size = len(vocab)\n",
    "# model = models.EmbeddingBagLinearModel(vocab_size, embedding_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "           Epoch 0: 100%|██████████| 340/340 [00:09<00:00, 37.23 batch/s, accurracy=0.5, loss=0.141]\n",
      "     Validation: 0: 100%|██████████| 76/76 [00:01<00:00, 46.61 batch/s, accurracy=0.02, loss=1.95]\n",
      "           Epoch 1: 100%|██████████| 340/340 [00:08<00:00, 40.31 batch/s, accurracy=0.25, loss=0.395]\n",
      "     Validation: 1: 100%|██████████| 76/76 [00:01<00:00, 46.99 batch/s, accurracy=0.04, loss=1.12]\n",
      "           Epoch 2: 100%|██████████| 340/340 [00:08<00:00, 41.61 batch/s, accurracy=0.75, loss=0.0532]\n",
      "     Validation: 2: 100%|██████████| 76/76 [00:01<00:00, 41.12 batch/s, accurracy=0.06, loss=1.39]\n",
      "           Epoch 3: 100%|██████████| 340/340 [00:08<00:00, 40.65 batch/s, accurracy=0.5, loss=0.135]\n",
      "     Validation: 3: 100%|██████████| 76/76 [00:01<00:00, 47.15 batch/s, accurracy=0.06, loss=1.29]\n",
      "           Epoch 4: 100%|██████████| 340/340 [00:08<00:00, 40.05 batch/s, accurracy=0.25, loss=0.199]\n",
      "     Validation: 4: 100%|██████████| 76/76 [00:01<00:00, 42.00 batch/s, accurracy=0.58, loss=0.694]\n",
      "           Epoch 5: 100%|██████████| 340/340 [00:08<00:00, 40.97 batch/s, accurracy=0.5, loss=0.154]\n",
      "     Validation: 5: 100%|██████████| 76/76 [00:01<00:00, 41.16 batch/s, accurracy=0.08, loss=1.39]\n",
      "           Epoch 6: 100%|██████████| 340/340 [00:08<00:00, 41.14 batch/s, accurracy=0.5, loss=0.0616]\n",
      "     Validation: 6: 100%|██████████| 76/76 [00:01<00:00, 45.03 batch/s, accurracy=0.28, loss=0.768]\n",
      "           Epoch 7: 100%|██████████| 340/340 [00:07<00:00, 44.84 batch/s, accurracy=0.5, loss=0.101]\n",
      "     Validation: 7: 100%|██████████| 76/76 [00:01<00:00, 48.85 batch/s, accurracy=0.2, loss=1.19]\n",
      "           Epoch 8: 100%|██████████| 340/340 [00:08<00:00, 41.87 batch/s, accurracy=0.5, loss=0.134]\n",
      "     Validation: 8: 100%|██████████| 76/76 [00:01<00:00, 40.52 batch/s, accurracy=0.18, loss=0.698]\n",
      "           Epoch 9: 100%|██████████| 340/340 [00:08<00:00, 41.42 batch/s, accurracy=0.75, loss=0.0342]\n",
      "     Validation: 9: 100%|██████████| 76/76 [00:01<00:00, 52.87 batch/s, accurracy=0.04, loss=2.59]\n",
      "          Epoch 10: 100%|██████████| 340/340 [00:07<00:00, 43.30 batch/s, accurracy=0.25, loss=0.31]\n",
      "    Validation: 10: 100%|██████████| 76/76 [00:01<00:00, 49.37 batch/s, accurracy=0.24, loss=1.59]\n",
      "          Epoch 11: 100%|██████████| 340/340 [00:07<00:00, 48.52 batch/s, accurracy=0.5, loss=0.153]\n",
      "    Validation: 11: 100%|██████████| 76/76 [00:01<00:00, 52.98 batch/s, accurracy=0.14, loss=1.17]\n",
      "          Epoch 12: 100%|██████████| 340/340 [00:07<00:00, 46.50 batch/s, accurracy=0.75, loss=0.0751]\n",
      "    Validation: 12: 100%|██████████| 76/76 [00:01<00:00, 42.00 batch/s, accurracy=0.1, loss=1.32]\n",
      "          Epoch 13: 100%|██████████| 340/340 [00:08<00:00, 39.36 batch/s, accurracy=0.5, loss=0.136]\n",
      "    Validation: 13: 100%|██████████| 76/76 [00:02<00:00, 37.25 batch/s, accurracy=0.44, loss=1.07]\n",
      "          Epoch 14: 100%|██████████| 340/340 [00:08<00:00, 38.51 batch/s, accurracy=0.5, loss=0.147]\n",
      "    Validation: 14: 100%|██████████| 76/76 [00:01<00:00, 44.01 batch/s, accurracy=0.1, loss=1.64]\n",
      "          Epoch 15: 100%|██████████| 340/340 [00:08<00:00, 41.18 batch/s, accurracy=0.25, loss=0.209]\n",
      "    Validation: 15: 100%|██████████| 76/76 [00:01<00:00, 45.02 batch/s, accurracy=0.36, loss=1.14]\n",
      "          Epoch 16: 100%|██████████| 340/340 [00:08<00:00, 39.44 batch/s, accurracy=0.75, loss=0.184]\n",
      "    Validation: 16: 100%|██████████| 76/76 [00:01<00:00, 44.69 batch/s, accurracy=0.2, loss=1.66]\n",
      "          Epoch 17: 100%|██████████| 340/340 [00:08<00:00, 39.48 batch/s, accurracy=0.75, loss=0.0966]\n",
      "    Validation: 17: 100%|██████████| 76/76 [00:01<00:00, 45.78 batch/s, accurracy=0.12, loss=2.45]\n",
      "          Epoch 18: 100%|██████████| 340/340 [00:08<00:00, 39.08 batch/s, accurracy=1, loss=0.0406]\n",
      "    Validation: 18: 100%|██████████| 76/76 [00:01<00:00, 40.85 batch/s, accurracy=0.16, loss=1.36]\n",
      "          Epoch 19: 100%|██████████| 340/340 [00:07<00:00, 42.90 batch/s, accurracy=0.5, loss=0.234]\n",
      "    Validation: 19: 100%|██████████| 76/76 [00:01<00:00, 46.89 batch/s, accurracy=0.42, loss=0.957]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "learning_rate = 5\n",
    "\n",
    "# Create the loss function weighted to inverse class distribution\n",
    "# loss_function = torch.nn.CrossEntropyLoss(weight=train_dataset.class_weights)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate a Stochastic Gradient Descent optimizer and \"Auto\" Learning Rate schedule.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"max\")\n",
    "\n",
    "# Tensorboard writing utility class.\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Perform Training\n",
    "for i in range(EPOCHS):\n",
    "    start_iter = len(train_dataloader) * i\n",
    "    train.train_epoch(i, model, optimizer, loss_function, train_dataloader, start_iter=start_iter, writer=writer)\n",
    "    validation_results = train.evaluate_epoch(i, model, loss_function, val_dataloader, writer)\n",
    "    # scheduler.step(validation_results[\"recall\"])\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3556 1144]\n",
      " [ 100   50]]\n",
      "Precision: 50 / (50 + 1144) = 0.0419\n",
      "Recall: 50 / (50 + 100) = 0.3333\n",
      "F1: 2 * (0.0419 * 0.3333) / (0.0419 + 0.3333) = 0.0744\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "labels = []\n",
    "for batch in dev_dataloader:\n",
    "    label, text, *_ = batch\n",
    "    pred_label = train.predict(model, text)\n",
    "    preds.append(pred_label)\n",
    "    labels.append(label.cpu().item())\n",
    "\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(cm)\n",
    "tn, fp, fn, tp = cm.ravel() # Extract the components\n",
    "\n",
    "# Calculate and print Precision\n",
    "precision_string = f\"{tp} / ({tp} + {fp})\"\n",
    "precision = round(eval(precision_string), 4)\n",
    "print(f\"Precision: {precision_string} = {precision}\")\n",
    "\n",
    "# Calculate and print Recall\n",
    "recall_string = f\"{tp} / ({tp} + {fn})\"\n",
    "recall = round(eval(recall_string), 4)\n",
    "print(f\"Recall: {recall_string} = {recall}\")\n",
    "\n",
    "# Calculate and print F1 Score\n",
    "f1_string = f\"2 * ({precision} * {recall}) / ({precision} + {recall})\"\n",
    "f1 = eval(f1_string)\n",
    "print(f\"F1: {f1_string} = {round(f1, 4)}\")\n",
    "\n",
    "\n",
    "print(f\"AP: {average_precision_score(labels, preds)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2c370576f24a547f2bd7f1badbf1e39ac1d010b680ddb4523878c870f4696a1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ir-classification-_Pgcz6ju-py3.9': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
