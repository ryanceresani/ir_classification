{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import io\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.utils import unicode_csv_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification import datasets, vocab, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab_from_tsv(\"../datasets/systematic_review/phase1.train.shuf.tsv\", [2], ngrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from torchtext.utils import (\n",
    "    unicode_csv_reader,\n",
    ")\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from typing import Callable, List\n",
    "from torch.utils import data\n",
    "import torch\n",
    "\n",
    "_default_tokenizer = get_tokenizer(\"basic_english\")\n",
    "DEFAULT_LABEL_PIPELINE = lambda x: x\n",
    "DEFAULT_TEXT_PIPELINE = lambda x: _default_tokenizer(x)\n",
    "\n",
    "\n",
    "def create_torch_dataloader(\n",
    "    dataset: data.Dataset,\n",
    "    vocab: Vocab,\n",
    "    label_pipeline: Callable = DEFAULT_LABEL_PIPELINE,\n",
    "    text_pipeline: Callable = DEFAULT_TEXT_PIPELINE,\n",
    "    **kwargs\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _collate_batch(batch):\n",
    "        label_list, text_list, offsets = [], [], [0]\n",
    "        for (_label, _text) in batch:\n",
    "            label_list.append(label_pipeline(_label))\n",
    "            processed_text = torch.tensor(vocab(text_pipeline(_text)), dtype=torch.int64)\n",
    "            text_list.append(processed_text)\n",
    "            offsets.append(processed_text.size(0))\n",
    "        label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "        text_list = torch.cat(text_list)\n",
    "        return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "    return data.DataLoader(dataset, collate_fn=_collate_batch, **kwargs)\n",
    "\n",
    "\n",
    "class TSVRawTextIterableDataset(data.IterableDataset):\n",
    "    def __init__(self, filepath: str, data_columns: List[int]):\n",
    "        self._number_of_items = _get_tsv_file_length(filepath)\n",
    "        self._iterator = _create_data_from_tsv(\n",
    "            filepath, data_column_indices=data_columns\n",
    "        )\n",
    "        self._current_position = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        item = next(self._iterator)\n",
    "        self._current_position += 1\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._number_of_items\n",
    "\n",
    "\n",
    "class TSVRawTextMapDataset(data.Dataset):\n",
    "    def __init__(self, filepath: str, data_columns: List[int]):\n",
    "        self._records = [\n",
    "            record\n",
    "            for record in _create_data_from_tsv(\n",
    "                filepath, data_column_indices=data_columns\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._records[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._records)\n",
    "\n",
    "\n",
    "def _create_data_from_tsv(data_path, data_column_indices):\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        reader = unicode_csv_reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            data = [row[i] for i in data_column_indices]\n",
    "            yield int(row[0]), \" \".join(data)\n",
    "\n",
    "\n",
    "def _get_tsv_file_length(data_path):\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        row_count = sum(1 for row in f)\n",
    "\n",
    "    return row_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = TSVRawTextIterableDataset(\"../datasets/systematic_review/phase1.train.shuf.tsv\", [2])\n",
    "data_map = TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.train.shuf.tsv\", [2])\n",
    "\n",
    "dl = create_torch_dataloader(data_map, vocab, label_pipeline=label_pipeline, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EmbeddingBagLinearModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, num_class: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        #print(embedded)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 2\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = EmbeddingBagLinearModel(vocab_size, emsize, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "weights=torch.tensor([1.0, 30.0])\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=10)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        \n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(0, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0   \n",
    "            start_time = time.time()\n",
    "            \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    \n",
    "    pred = []\n",
    "    true = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            pred.append(predicted_label.argmax(1))\n",
    "            true.append(label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return torch.cat(pred).numpy(), torch.cat(true).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = TSVRawTextIterableDataset(\"../datasets/systematic_review/phase1.train.shuf.tsv\", [2])\n",
    "data_map = TSVRawTextMapDataset(\"../datasets/systematic_review/phase1.dev.shuf.tsv\", [2])\n",
    "label_pipeline = lambda x: x if x > 0 else 0\n",
    "text_pipeline = lambda x: list(ngrams_iterator(tokenizer(x), 2))\n",
    "train_dl = create_torch_dataloader(data_map, vocab, label_pipeline=label_pipeline, text_pipeline=text_pipeline)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2c370576f24a547f2bd7f1badbf1e39ac1d010b680ddb4523878c870f4696a1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ir-classification-_Pgcz6ju-py3.9': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
